# Text normalization

The MVP model is [on huggingface](https://huggingface.co/saarus72/russian_text_normalizer).

### Why

A pet project as the (only) [other solution](https://github.com/snakers4/russian_stt_text_normalization) does not seem to be maintainable by either owner or others. Some others do exist and are, like, *bad*.

> E.g. for input `Ñ ĞºÑƒĞ¿Ğ¸Ğ» iphone 10X Ğ·Ğ° 14990 Ñ€ÑƒĞ± Ğ±ĞµĞ· 3-x Ñ‡Ğ°ÑĞ¾Ğ² Ğ¿Ğ¾Ğ»Ğ´ĞµĞ½ÑŒ Ğ¸ Ñ‚.Ğ´.` output of
> * [text-normalization-ru-terrible](https://huggingface.co/maximxls/text-normalization-ru-terrible) is `Ñ ĞºÑƒĞ¿Ğ¸Ğ» Ğ°Ğ¹Ñ„Ğ¾Ğ½ ÑÑ‚Ğ¾ Ğ¸ĞºÑ Ğ·Ğ° Ñ‚Ñ‹ÑÑÑ‡Ñƒ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑÑ‚Ğ° Ğ´ĞµĞ²ÑĞ½Ğ¾ÑÑ‚Ğ¾ Ñ€ÑƒĞ±Ğ»Ğµ Ğ±ĞµĞ· Ñ‚Ñ€ĞµÑ‚ÑŒĞ¸Ñ… Ñ‡Ğ°ÑĞ¾Ğ² Ğ¿Ğ¾Ğ»`,
> * [text-normalization-ru-new](https://huggingface.co/alexue4/text-normalization-ru-new) is `Ñ ĞºÑƒĞ¿Ğ¸Ğ» Ğ¸Ñ„Ğ¾Ğ½ Ğ´ĞµÑÑÑ‚ÑŒ Ğ¸ĞºÑ Ğ·Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ Ñ‚Ñ‹ÑÑÑ‡ Ğ´ĞµĞ²ÑÑ‚ÑŒ`.
> * [russian_stt_text_normalization](https://github.com/snakers4/russian_stt_text_normalization) itself is `Ñ ĞºÑƒĞ¿Ğ¸Ğ» Ğ¸Ñ„Ğ¾Ğ½ Ğ´ĞµÑÑÑ‚ÑŒ ĞºÑ Ğ·Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ´ĞµĞ²ÑÑ‚ÑŒ Ğ´ĞµĞ²ÑÑ‚ÑŒ Ğ½Ğ¾Ğ»ÑŒ Ñ€ÑƒĞ±Ğ»ĞµĞ¹ Ğ±ĞµĞ· Ñ‚Ñ€ĞµÑ‚ÑŒĞ¸ Ñ‡Ğ°ÑĞ¾Ğ² Ğ¿Ğ¾Ğ»Ğ´ĞµĞ½ÑŒ Ğ¸ Ñ‚.Ğ´.`


from normalizer import Normalizer
normlizer = Normalizer(jit_model="/models/jit_s2s.pt")
normlizer.norm_text(text)

### Do I have a plan

I from where I stand see these steps:

1. Get a dataset.
    > Notebooks to [find](./work/dataset/1_find_numbers.ipynb) and to [itn](./work/dataset/2b_inverse_normalize.ipynb) texts.
    1. Download any vast (informal?) russian raw text corpus. Could be
        * [IlyaGusev/ficbook](https://huggingface.co/datasets/IlyaGusev/ficbook),
        * [IlyaGusev/librusec_full](https://huggingface.co/datasets/IlyaGusev/librusec_full), or
        * [Taiga Corpus](https://tatianashavrina.github.io/taiga_site).
    1. Find occurances w/ regexp patterns like `r"Ğ´Ğ²Ğ°Ğ´Ñ†Ğ°Ñ‚\S+"`, 
    1. Make sure there is nothing but cyrillic.
    1. Make inverse text normalization (that task is more straightforward and many good solutions do exist).
        * Used ~~[NeMo Text Processing](https://github.com/NVIDIA/NeMo-text-processing)~~ [another python package](https://github.com/flockentanz/word_to_number_ru) with some additions.
    1. Polish things roughly like balance (as `Ğ´Ğ²Ğ°` seems to be *far* more common than `Ğ´Ğ²ÑƒĞ¼ÑÑÑ‚Ğ°Ğ¼Ğ¸`), get rid of ITN mistakes etc.
1. Train an MVP.
    > Notebook to [train](./work/train/train.ipynb) a model.
    1. Get a relatively big LLM as we are going to prune it then (and to onnx it as well so that the resulting performance is compatible with the solution I've mantioned).
        * Seems to be [ai-forever/FRED-T5-1.7B](https://huggingface.co/ai-forever/FRED-T5-1.7B) as it is encoder-decoder, trainable on single **RTX3060 12Gb** and good enough to get an MVP.
            > Turned out that 12 Gb is enough to inference it only so I've trained [ai-forever/FRED-T5-large](https://huggingface.co/ai-forever/FRED-T5-large).
    1. Train, like, any barely working model.
        * Several attempts are required as it is not clear which prompt is better. May be
            ```
            <SC1>Ğ‘Ñ‹Ğ»Ğ¾ Ñƒ Ğ¾Ñ‚Ñ†Ğ° [3]<extra_id_0> ÑÑ‹Ğ½Ğ° Ğ¸ [2-3]<extra_id_1> Ğ¿Ğ¸Ğ´Ğ¶Ğ°ĞºĞ° Ñ Ğ±Ğ»Ñ‘ÑÑ‚ĞºĞ°Ğ¼Ğ¸.
            ```
            > Turned out the pattern below works well so I've made no experiments here.
    1. Test and analyze.
    1. ~~Regret deeply.~~
1. To obtain a dataset of a better quality, we want to ask really big smart ass LLM to **(not inverse!)** normalize texts during the training. It may benefit in
    * having latin staff normalization (abbreviations, brands, urls etc.),
    * cover non-trivial numbers like dates `Ğ² Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´ 20-24.10.23 Ğ¾Ñ‚ĞºĞ»ÑÑ‡Ğ°Ñ‚ Ğ²ÑÑ‘ Ğ¸ ÑÑ€Ğ°Ğ·Ñƒ` as we are unlikely to construct such strings on owr own, and
    * balance numbers as we seem to may change **some** digits freely to have a *bigger* number (from `Ğ´Ğ¾ 3-Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ` to `Ğ´Ğ¾ 1488-Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ` but not  `Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ» 3 Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ` to `Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ» 1488 Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ`; increase on a multiple of 100 looks nice).
1. Finetune again.
1. GOTO 1 or 3.

## Inverse Text Normalization

There are but a few packages from namely 
* NVidia's [NeMo](https://github.com/NVIDIA/NeMo-text-processing),
* [Oknolaz](https://github.com/Oknolaz/Russian_w2n),
* [SergeyShk](https://github.com/SergeyShk/Word-to-Number-Russian) and its forks from
    * [averkij](https://github.com/averkij/Word-to-Number-Russian) and
    * [flockentanz](https://github.com/flockentanz/word_to_number_ru).

**NeMo** works well but tends to miss many cases I won't have missed (see the comparison table below). I used it as the first attempt but did my research then.

**Oknolaz** needs to be fed with extracted numbers only and does many mistakes in that case even so bad choice for us.

**SergeyShk** does either
* `replace_groups` â€” `Ñ‚Ñ‹ÑÑÑ‡Ğ° ÑÑ‚Ğ¾` to `1100` but `ÑÑ‚Ğ¾ Ğ´Ğ²ĞµÑÑ‚Ğ¸ Ñ‚Ñ€Ğ¸ÑÑ‚Ğ°` to `400` or
* `replace` â€” `ÑÑ‚Ğ¾ Ğ´Ğ²ĞµÑÑ‚Ğ¸ Ñ‚Ñ€Ğ¸ÑÑ‚Ğ°` to `100 200 300` but `Ñ‚Ñ‹ÑÑÑ‡Ğ° ÑÑ‚Ğ¾` to `1000 100`.

It is obvious that addition should be done on decreasing values only so there are some forks to fix it (the overall code is a mess so that I didn't want to do it myself anyway).

**averkij** and **flockentanz** work fine both but have some bugs so I took the second one and fixed them. Also I cover cases like `Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ¾Ğ¹` and `Ğ¾Ğ´Ğ½Ğ° Ñ†ĞµĞ»Ğ°Ñ Ğ´Ğ²Ğµ Ğ´ĞµÑÑÑ‚Ñ‹Ñ…`.

| Original | ğŸŸ¡ NeMo TP | ğŸ”´ Oknolaz `replace` | ğŸ”´ SergeyShk `replace_groups` | ğŸ”´ SergeyShk `replace` | ğŸ”´ averkij `replace` | ğŸ”´ flockentanz `replace_groups_sa` | ğŸŸ¢ flockentanz fixed |
|--|--|--|--|--|--|--|--|
| `ÑÑ‚Ğ¾ Ğ´Ğ²ĞµÑÑ‚Ğ¸ Ñ‚Ñ€Ğ¸ÑÑ‚Ğ° Ğ´Ğ° Ñ…Ğ¾Ñ‚ÑŒ Ñ‚Ñ‹ÑÑÑ‡Ñƒ Ñ€Ğ°Ğ·` | ğŸŸ¢`100 200 300 Ğ´Ğ° Ñ…Ğ¾Ñ‚ÑŒ 1000 Ñ€Ğ°Ğ·` | ğŸ”´`600000` | ğŸ”´`400 Ğ´Ğ° Ñ…Ğ¾Ñ‚ÑŒ 1000 Ñ€Ğ°Ğ·` | ğŸŸ¢`100 200 300 Ğ´Ğ° Ñ…Ğ¾Ñ‚ÑŒ 1000 Ñ€Ğ°Ğ·` | ğŸ”´`10200 300 Ğ´Ğ° Ñ…Ğ¾Ñ‚ÑŒ 1000 Ñ€Ğ°Ğ·` | ğŸŸ¢`100 200 300 Ğ´Ğ° Ñ…Ğ¾Ñ‚ÑŒ 1000 Ñ€Ğ°Ğ·` | ğŸŸ¢`100 200 300 Ğ´Ğ° Ñ…Ğ¾Ñ‚ÑŒ 1000 Ñ€Ğ°Ğ·` |
| `Ñ‚Ñ‹ÑÑÑ‡Ğ° ÑÑ‚Ğ¾` | ğŸŸ¢`1100` | ğŸŸ¢`1100` | ğŸŸ¢`1100` | ğŸ”´`1000 100` | ğŸŸ¢`1100` | ğŸŸ¢`1100` | ğŸŸ¢`1100` |
| `Ñ Ğ²Ğ¸Ğ´ĞµĞ» ÑÑ‚Ğ¾-Ğ´Ğ²ĞµÑÑ‚Ğ¸ ÑˆÑ‚ÑƒĞº` | ğŸŸ¡`Ñ Ğ²Ğ¸Ğ´ĞµĞ» ÑÑ‚Ğ¾-Ğ´Ğ²ĞµÑÑ‚Ğ¸ ÑˆÑ‚ÑƒĞº` | ğŸ”´`300` | ğŸŸ¢`Ñ Ğ²Ğ¸Ğ´ĞµĞ» 100-200 ÑˆÑ‚ÑƒĞº` | ğŸŸ¢`Ñ Ğ²Ğ¸Ğ´ĞµĞ» 100-200 ÑˆÑ‚ÑƒĞº` | ğŸŸ¢`Ñ Ğ²Ğ¸Ğ´ĞµĞ» 100-200 ÑˆÑ‚ÑƒĞº` | ğŸŸ¢`Ñ Ğ²Ğ¸Ğ´ĞµĞ» 100-200 ÑˆÑ‚ÑƒĞº` | ğŸŸ¢`Ñ Ğ²Ğ¸Ğ´ĞµĞ» 100-200 ÑˆÑ‚ÑƒĞº` |
| `Ğ²Ğ¾ÑĞµĞ¼ÑŒ Ğ´ĞµĞ²ÑÑ‚ÑŒÑĞ¾Ñ‚ Ğ´Ğ²Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ Ğ´Ğ²Ğ° Ğ¿ÑÑ‚ÑŒ Ğ¿ÑÑ‚ÑŒ Ğ¿ÑÑ‚ÑŒ Ñ‚Ñ€Ğ¸Ğ´Ñ†Ğ°Ñ‚ÑŒ Ğ¿ÑÑ‚ÑŒ Ñ‚Ñ€Ğ¸Ğ´Ñ†Ğ°Ñ‚ÑŒ Ğ¿ÑÑ‚ÑŒ, Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ½Ğ¸Ñ‚ÑŒ, Ñ‡ĞµĞ¼ Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ` | ğŸŸ¡`Ğ²Ğ¾ÑĞµĞ¼ÑŒ 922 Ğ¿ÑÑ‚ÑŒ Ğ¿ÑÑ‚ÑŒ Ğ¿ÑÑ‚ÑŒ 35 35 , Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ½Ğ¸Ñ‚ÑŒ, Ñ‡ĞµĞ¼ Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ` | ğŸ”´`8` | ğŸ”´`115, Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ½Ğ¸Ñ‚ÑŒ, Ñ‡ĞµĞ¼ Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ` | ğŸ”´`8 900 20 2 5 5 5 30 5 30 5, Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ½Ğ¸Ñ‚ÑŒ, Ñ‡ĞµĞ¼ Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ` | ğŸŸ¢`8 922 5 5 5 35 35, Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ½Ğ¸Ñ‚ÑŒ, Ñ‡ĞµĞ¼ Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ` | ğŸŸ¢`8 922 5 5 5 35 35, Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ½Ğ¸Ñ‚ÑŒ, Ñ‡ĞµĞ¼ Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ` | ğŸŸ¢`8 922 5 5 5 35 35, Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ½Ğ¸Ñ‚ÑŒ, Ñ‡ĞµĞ¼ Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ` |
| `Ñ‚Ñ€Ğ¸ Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°` | ğŸŸ¡`Ñ‚Ñ€Ğ¸ Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°` | ğŸ”´`3` | ğŸŸ¡`3 Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°` | ğŸŸ¡`3 Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°` | ğŸŸ¢`3.5 Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°` | ğŸŸ¡`3 Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°` | ğŸŸ¢`3.5 Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°` |
| `Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ ÑÑ‚Ğ¾ Ñ‚Ñ‹ÑÑÑ‡ ÑÑ‚Ğ¾ Ğ·Ğ°Ğ¹Ñ†ĞµĞ²` | ğŸŸ¢`1100100 Ğ·Ğ°Ğ¹Ñ†ĞµĞ²` | âŒ`list index out of range` | ğŸ”´`1000100100 Ğ·Ğ°Ğ¹Ñ†ĞµĞ²` | ğŸ”´`1000000 100000 100 Ğ·Ğ°Ğ¹Ñ†ĞµĞ²` | `1100100 Ğ·Ğ°Ğ¹Ñ†ĞµĞ²` | ğŸ”´`1000100100 Ğ·Ğ°Ğ¹Ñ†ĞµĞ²` | ğŸŸ¢`1100100 Ğ·Ğ°Ğ¹Ñ†ĞµĞ²` |
| `Ğ¾Ğ´Ğ½Ğ¸ Ğ´Ğ²Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿ÑÑ‚Ñ‘Ñ€ĞºĞ¸` | ğŸŸ¡`Ğ¾Ğ´Ğ½Ğ¸ Ğ´Ğ²Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿ÑÑ‚Ñ‘Ñ€ĞºĞ¸` | ğŸŸ¡`No valid number words found! ...` | ğŸŸ¡`1 Ğ´Ğ²Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ½Ğ¸ 1 Ğ¿ÑÑ‚Ñ‘Ñ€ĞºĞ¸` | ğŸŸ¡`1 Ğ´Ğ²Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ½Ğ¸ 1 Ğ¿ÑÑ‚Ñ‘Ñ€ĞºĞ¸` | ğŸŸ¡`1 Ğ´Ğ²Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ½Ğ¸ 1 Ğ¿ÑÑ‚Ñ‘Ñ€ĞºĞ¸` | ğŸŸ¡`1 Ğ´Ğ²Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ½Ğ¸ 1 Ğ¿ÑÑ‚Ñ‘Ñ€ĞºĞ¸` | ğŸŸ¡`1 Ğ´Ğ²Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ½Ğ¸ 1 Ğ¿ÑÑ‚Ñ‘Ñ€ĞºĞ¸` |
| `Ğ±ĞµĞ· Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹ Ğ´Ğ²Ğ°` |ğŸŸ¢ `01:59` | ğŸ”´`2` | ğŸŸ¢`Ğ±ĞµĞ· 1 Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹ 2` | ğŸŸ¢`Ğ±ĞµĞ· 1 Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹ 2` | ğŸŸ¢`Ğ±ĞµĞ· 1 Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹ 2` | ğŸŸ¢`Ğ±ĞµĞ· 1 Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹ 2` | ğŸŸ¢`Ğ±ĞµĞ· 1 Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹ 2` |
| `Ğ²Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ°Ñ‡Ğ° Ğ¿ÑÑ‚ÑŒ ÑĞ¾Ñ‚Ğ¾Ğº` | ğŸŸ¡`Ğ²Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ°Ñ‡Ğ° Ğ¿ÑÑ‚ÑŒ ÑĞ¾Ñ‚Ğ¾Ğº` | ğŸ”´`5` | ğŸŸ¢`2 Ğ´Ğ°Ñ‡Ğ° 5 ÑĞ¾Ñ‚Ğ¾Ğº` | ğŸŸ¢`2 Ğ´Ğ°Ñ‡Ğ° 5 ÑĞ¾Ñ‚Ğ¾Ğº` | ğŸŸ¢`2 Ğ´Ğ°Ñ‡Ğ° 5 ÑĞ¾Ñ‚Ğ¾Ğº` | ğŸŸ¢`2 Ğ´Ğ°Ñ‡Ğ° 5 ÑĞ¾Ñ‚Ğ¾Ğº` | ğŸŸ¢`2 Ğ´Ğ°Ñ‡Ğ° 5 ÑĞ¾Ñ‚Ğ¾Ğº` |
| `Ğ´Ğ²ĞµÑÑ‚Ğ¸ Ğ¿ÑÑ‚ÑŒĞ´ĞµÑÑÑ‚ Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ¾Ğ¹ Ñ‚Ñ‹ÑÑÑ‡ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑĞ¾Ğ»Ğ´Ğ°Ñ‚ Ğ˜Ñ€Ğ°ĞºĞ°` | ğŸŸ¡`250 Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ¾Ğ¹ 1000 Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑĞ¾Ğ»Ğ´Ğ°Ñ‚ Ğ˜Ñ€Ğ°ĞºĞ°` | ğŸ”´`250000` | ğŸŸ¡`250 Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ¾Ğ¹ 1000 Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑĞ¾Ğ»Ğ´Ğ°Ñ‚ Ğ˜Ñ€Ğ°ĞºĞ°` | ğŸ”´`200 50 Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ¾Ğ¹ 1000 Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑĞ¾Ğ»Ğ´Ğ°Ñ‚ Ğ˜Ñ€Ğ°ĞºĞ°` | ğŸ”´`2050000.5 Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑĞ¾Ğ»Ğ´Ğ°Ñ‚ Ğ˜Ñ€Ğ°ĞºĞ°` | ğŸŸ¡`250 Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ¾Ğ¹ 1000 Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑĞ¾Ğ»Ğ´Ğ°Ñ‚ Ğ˜Ñ€Ğ°ĞºĞ°` | ğŸŸ¢`250500 Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ½Ñ‹Ñ… ÑĞ¾Ğ»Ğ´Ğ°Ñ‚ Ğ˜Ñ€Ğ°ĞºĞ°` |
| `Ğ½Ğ¾Ğ»ÑŒ Ñ†ĞµĞ»Ñ‹Ñ… Ğ½Ğ¾Ğ»ÑŒ Ğ´ĞµÑÑÑ‚Ñ‹Ñ… Ğ¼Ğ¸Ğ½ÑƒÑ Ğ´Ğ²Ğµ Ñ†ĞµĞ»Ñ‹Ñ… ÑˆĞµÑÑ‚ÑŒ ÑĞ¾Ñ‚Ñ‹Ñ…` | ğŸŸ¢`0,0 -2,06` | ğŸŸ¡`Redundant number word! ...` | ğŸ”´`0 Ñ†ĞµĞ»Ñ‹Ñ… 0.0 Ğ¼Ğ¸Ğ½ÑƒÑ 2 Ñ†ĞµĞ»Ñ‹Ñ… 0.06` | ğŸ”´`0 Ñ†ĞµĞ»Ñ‹Ñ… 0.0 Ğ¼Ğ¸Ğ½ÑƒÑ 2 Ñ†ĞµĞ»Ñ‹Ñ… 0.06` | ğŸ”´`0 Ñ†ĞµĞ»Ñ‹Ñ… 0.0 Ğ¼Ğ¸Ğ½ÑƒÑ 2 Ñ†ĞµĞ»Ñ‹Ñ… 0.06` | ğŸ”´`0 Ñ†ĞµĞ»Ñ‹Ñ… 0.0 Ğ¼Ğ¸Ğ½ÑƒÑ 2 Ñ†ĞµĞ»Ñ‹Ñ… 0.06` | ğŸŸ¢`0 Ğ¼Ğ¸Ğ½ÑƒÑ 2.06` |