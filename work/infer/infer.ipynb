{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ac75be-4e65-4949-bfed-83912bf307da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install gradio sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff11d0-f103-4139-b94b-6dab26563795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, T5ForConditionalGeneration\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b2e67-8a5e-4fde-a87b-7289236ec56d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# device = \"cuda:0\"\n",
    "device = \"cpu\"\n",
    "HOST_IP = \"192.168.31.167\"\n",
    "GRADIO_PORT = 7860"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f2d4b-8f1e-4668-99a6-20a34d35e3ec",
   "metadata": {},
   "source": [
    "## FRED-T5-large-FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00325c3d-d348-49e9-9c8d-7b387d1f7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/home/jovyan/wdc1/models/FRED-T5-large\"\n",
    "path = \"/home/jovyan/models/3_fred-t5/checkpoint-11000\"\n",
    "path = \"/home/jovyan/models/7_fred-t5-large/checkpoint-35000\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(path, eos_token='</s>')\n",
    "model = T5ForConditionalGeneration.from_pretrained(path).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18857a-faf0-42b6-9df4-a31ba2f360c2",
   "metadata": {},
   "source": [
    "## ruT5-base-FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8990431a-fb43-4a87-8bef-34fa0644dfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "path = \"/home/jovyan/models/8_ruT5-base/checkpoint-17000/\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(path).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(path)\n",
    "tokenizer.add_tokens(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27a9773-255f-4be0-b896-274d6dfb2ea3",
   "metadata": {},
   "source": [
    "## Common code then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df65fc-34da-49ea-8d06-f26647cadea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    input_ids = torch.tensor([tokenizer.encode(text)]).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids, max_new_tokens=50, eos_token_id=tokenizer.eos_token_id, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0][1:])\n",
    "\n",
    "\n",
    "predict(\"<SC1>Было у отца [3]<extra_id_0> сына, но не было даже [2- 3]<extra_id_1> пиджаков с блёстками за [142 990]<extra_id_2> руб.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8bd54-af8f-4dcb-a2b5-39eb96f5439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"examples.json\") as f:\n",
    "    test_examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3617f748-5fa9-40f3-a8d6-a366c6ce8d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re_tokens = re.compile(r\"[а-яА-Я]+\\s*|\\d+(?:\\.\\d+)?\\s*|[^а-яА-Я\\d\\s]+\\s*\")\n",
    "re_tokens = re.compile(r\"(?:[.,!?]|[а-яА-Я]\\S*|\\d\\S*(?:\\.\\d+)?|[^а-яА-Я\\d\\s]+)\\s*\")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(re_tokens, text)\n",
    "\n",
    "\n",
    "def strip_numbers(s):\n",
    "    result = []\n",
    "    for part in s.split():\n",
    "        if part.isdigit():\n",
    "            while len(part) > 3:\n",
    "                result.append(part[:- 3 * ((len(part) - 1) // 3)])\n",
    "                part = part[- 3 * ((len(part) - 1) // 3):]\n",
    "            if part:\n",
    "                result.append(part)\n",
    "        else:\n",
    "            result.append(part)\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "def construct_prompt(text):\n",
    "    result = \"<SC1>\"\n",
    "    etid = 0\n",
    "    token_to_add = \"\"\n",
    "    for token in tokenize(text) + [\"\"]:\n",
    "        if not re.search(\"[a-zA-Z\\d]\", token):\n",
    "            if token_to_add:\n",
    "                end_match = re.search(r\"(.+?)(\\W*)$\", token_to_add, re.M).groups()\n",
    "                result += f\"[{strip_numbers(end_match[0])}]<extra_id_{etid}>{end_match[1]}\"\n",
    "                etid += 1\n",
    "                token_to_add = \"\"\n",
    "            result += token\n",
    "        else:\n",
    "            token_to_add += token\n",
    "    return result\n",
    "\n",
    "\n",
    "construct_prompt('я купил iphone 12X за 142 990 руб без 3-x часов 12:00, и т.д.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a10cf6b-69a0-480b-8b95-5bdfffca669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_answer(prompt:str, prediction:str) -> str:\n",
    "    replaces = []\n",
    "    re_prompt = re.compile(r\"\\[([^\\]]+)\\]<extra_id_(\\d+)>\")\n",
    "    re_pred = re.compile(r\"\\<extra_id_(\\d+)\\>(.+?)(?=\\<extra_id_\\d+\\>|</s>)\")\n",
    "    pred_data = {}\n",
    "    for match in re.finditer(re_pred, prediction.replace(\"\\n\", \" \")):\n",
    "        pred_data[match[1]] = match[2].strip()\n",
    "    while match := re.search(re_prompt, prompt):\n",
    "        replace = pred_data.get(match[2], match[1])\n",
    "        prompt = prompt[:match.span()[0]] + replace + prompt[match.span()[1]:]\n",
    "    return prompt.replace(\"<SC1>\", \"\")\n",
    "        \n",
    "construct_answer(\n",
    "    '<SC1>Было у отца [3]<extra_id_0> сына. Старшему было [35]<extra_id_1>, среднему - не меньше [33]<extra_id_2>, а младший на [4]<extra_id_3> младше всех. Бывает.',\n",
    "    \"\"\"<extra_id_0>  три\n",
    " <extra_id_1>  тридцать пять\n",
    " <extra_id_2>  тридцати трех\n",
    " <extra_id_3>  четыре\n",
    "</s>\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0af086-7e52-4ecc-8535-c99a4a7936e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(message, history):\n",
    "    prompt = construct_prompt(message)\n",
    "    yield f\"```Prompt:\\n{prompt}\\nPrediction:\\n...```\\n...\"\n",
    "    prediction = predict(prompt)\n",
    "    answer = construct_answer(prompt, prediction)\n",
    "    yield f\"Prompt:\\n```{prompt}```\\nPrediction:\\n```\\n{prediction}\\n```\\n{answer}\"\n",
    "\n",
    "\n",
    "demo = gr.ChatInterface(norm, stop_btn=None, examples=list(test_examples.keys())).queue()\n",
    "demo.launch(inline=False, server_name=\"0.0.0.0\", server_port=GRADIO_PORT, inbrowser=True)\n",
    "IFrame(src=f\"http://{HOST_IP}:{GRADIO_PORT}\", width='100%', height='500px')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe5608-2b20-44bc-8599-4f36c24b7cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# found bad results with batch generation on encoder-decoder architectures surprisingly so one by one here\n",
    "for lm_text, gt in test_examples.items():\n",
    "    prompt = construct_prompt(lm_text)\n",
    "    prediction = predict(prompt)\n",
    "    answer = construct_answer(prompt, prediction)\n",
    "    if gt == answer:\n",
    "        print(f\"{gt}\\n\")\n",
    "    else:\n",
    "        print(f\"{lm_text}\\n{prompt}\\n{gt}\\n{answer}\\n{prediction}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
