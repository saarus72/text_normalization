{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff11d0-f103-4139-b94b-6dab26563795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, T5ForConditionalGeneration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8bd54-af8f-4dcb-a2b5-39eb96f5439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = {\n",
    "    'я купил iphone 12X за 142 990 руб без 3-x часов полдень и т.д.': \n",
    "        \"я купил айфон двенадцать икс за сто сорок две тысячи девятьсот девяносто руб без трёх часов полдень и т.д.\",\n",
    "    'я купил айфон за 14 970 рублей': \n",
    "        \"я купил айфон за четырнадцать тысяч девятьсот семьдесят рублей\",\n",
    "    \"Временами я думаю, какое применение найти тем 14 697 рублям, что лежат уже больше 33 лет?\": \n",
    "        \"Временами я думаю, какое применение найти тем четырнадцати тысячам шестистам девяносто семи рублям, что лежат уже больше тридцати трёх лет?\",\n",
    "    \"Было у отца 3 сына, но не было даже 2-3 пиджаков с блёстками за 142 990 рублей.\": \n",
    "        \"Было у отца три сына, но не было даже двух-трёх пиджаков с блёстками за сто сорок две тысячи девятьсто девяносто рублей.\",\n",
    "    \"В школе у меня одни 5.\": \n",
    "        \"В школе у меня одни пятёрки.\",\n",
    "    'Было у отца 3 сына. Старшему было 35, среднему - не меньше 33, а младший на 4 младше всех. Бывает.': \n",
    "        \"Было у отца три сына. Старшему было тридцать пять, среднему - не меньше тридцати трех, а младший на четыре младше всех. Бывает.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3617f748-5fa9-40f3-a8d6-a366c6ce8d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# re_tokens = re.compile(r\"[а-яА-Я]+\\s*|\\d+(?:\\.\\d+)?\\s*|[^а-яА-Я\\d\\s]+\\s*\")\n",
    "re_tokens = re.compile(r\"(?:[.,!?]|[а-яА-Я]\\S*|\\d\\S*(?:\\.\\d+)?|[^а-яА-Я\\d\\s]+)\\s*\")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(re_tokens, text)\n",
    "\n",
    "\n",
    "def strip_numbers(s):\n",
    "    return \" \".join(((\" \".join(part) if part.isdigit() else part) for part in s.split()))\n",
    "\n",
    "\n",
    "def strip_numbers(s):\n",
    "    result = []\n",
    "    for part in s.split():\n",
    "        if part.isdigit():\n",
    "            while len(part) > 3:\n",
    "                result.append(part[:- 3 * ((len(part) - 1) // 3)])\n",
    "                part = part[- 3 * ((len(part) - 1) // 3):]\n",
    "            if part:\n",
    "                result.append(part)\n",
    "        else:\n",
    "            result.append(part)\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "def construct_prompt(text):\n",
    "    result = \"<SC1>\"\n",
    "    etid = 0\n",
    "    token_to_add = \"\"\n",
    "    for token in tokenize(text) + [\"\"]:\n",
    "        if not re.search(\"[a-zA-Z\\d]\", token):\n",
    "            if token_to_add:\n",
    "                end_match = re.search(r\"(.+?)(\\W*)$\", token_to_add, re.M).groups()\n",
    "                result += f\"[{strip_numbers(end_match[0])}]<extra_id_{etid}>{end_match[1]}\"\n",
    "                etid += 1\n",
    "                token_to_add = \"\"\n",
    "            result += token\n",
    "        else:\n",
    "            token_to_add += token\n",
    "    return result\n",
    "\n",
    "\n",
    "construct_prompt('я купил iphone 12X за 142 990 руб без 3-x часов 12:00, и т.д.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a10cf6b-69a0-480b-8b95-5bdfffca669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def construct_answer(prompt:str, prediction:str) -> str:\n",
    "    replaces = []\n",
    "    re_prompt = re.compile(r\"\\[([^\\]]+)\\]<extra_id_(\\d+)>\")\n",
    "    re_pred = re.compile(r\"\\<extra_id_(\\d+)\\>(.+?)(?=\\<extra_id_\\d+\\>|</s>)\")\n",
    "    pred_data = {}\n",
    "    for match in re.finditer(re_pred, prediction.replace(\"\\n\", \" \")):\n",
    "        pred_data[match[1]] = match[2].strip()\n",
    "    while match := re.search(re_prompt, prompt):\n",
    "        replace = pred_data.get(match[2], match[1])\n",
    "        prompt = prompt[:match.span()[0]] + replace + prompt[match.span()[1]:]\n",
    "    return prompt.replace(\"<SC1>\", \"\")\n",
    "        \n",
    "construct_answer(\n",
    "    '<SC1>Было у отца [3]<extra_id_0> сына. Старшему было [35]<extra_id_1>, среднему - не меньше [33]<extra_id_2>, а младший на [4]<extra_id_3> младше всех. Бывает.',\n",
    "    \"\"\"<extra_id_0>  три\n",
    " <extra_id_1>  тридцать пять\n",
    " <extra_id_2>  тридцати трех\n",
    " <extra_id_3>  четыре\n",
    "</s>\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f2d4b-8f1e-4668-99a6-20a34d35e3ec",
   "metadata": {},
   "source": [
    "## FRED-T5-large-FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b2e67-8a5e-4fde-a87b-7289236ec56d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device='cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00325c3d-d348-49e9-9c8d-7b387d1f7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/home/jovyan/wdc1/models/FRED-T5-large\"\n",
    "path = \"/home/jovyan/models/3_fred-t5/checkpoint-11000\"\n",
    "path = \"/home/jovyan/models/6_fred-t5/checkpoint-5000\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(path, eos_token='</s>')\n",
    "model = T5ForConditionalGeneration.from_pretrained(path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df65fc-34da-49ea-8d06-f26647cadea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    input_ids = torch.tensor([tokenizer.encode(text)]).to(device)\n",
    "    outputs = model.generate(input_ids, eos_token_id=tokenizer.eos_token_id, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe5608-2b20-44bc-8599-4f36c24b7cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# found bad results with batch generation on encoder-decoder architectures surprisingly so one by one here\n",
    "for lm_text, gt in test_examples.items():\n",
    "    prompt = construct_prompt(lm_text)\n",
    "    pred = construct_answer(prompt, predict(prompt))\n",
    "    if gt == pred:\n",
    "        print(f\"{gt}\\n\")\n",
    "    else:\n",
    "        print(f\"{lm_text}\\n{prompt}\\n{gt}\\n{pred}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c25be0-7b3b-48be-bece-3a3753717b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    lm_text = input()\n",
    "    if not lm_text:\n",
    "        break\n",
    "    prompt = construct_prompt(lm_text)\n",
    "    pred = construct_answer(prompt, predict(prompt))\n",
    "    print(f\"{lm_text}\\n{prompt}\\n{pred}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
