{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24a48cdd-f6ec-4663-afa2-2d07c73e1855",
   "metadata": {},
   "source": [
    "# Find numbers\n",
    "\n",
    "among plain text corpus.\n",
    "\n",
    "We are looking for numbers now. First, we download that.\n",
    "\n",
    "* [IlyaGusev/librusec](https://huggingface.co/datasets/IlyaGusev/librusec)\n",
    "* [IlyaGusev/pikabu](https://huggingface.co/datasets/IlyaGusev/pikabu)\n",
    "\n",
    "> `pip install datasets zstandard jsonlines pysimdjson` is advised.\n",
    "\n",
    "The most simple way is to execute `git clone https://huggingface.co/datasets/IlyaGusev/librusec` eg.\n",
    "\n",
    "> One is necessarily to turn on an lfs support though.\n",
    "> \n",
    "> ```\n",
    "> curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "> sudo apt-get install git-lfs\n",
    "> git lfs install\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d603b6dc-70ea-4272-a7cf-a4636c606fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pprint import pprint, pformat\n",
    "from tqdm.notebook import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee54e2-e2f2-4199-a6fd-69dc18ebb06b",
   "metadata": {},
   "source": [
    "Now we use the most direct approach and just morph a number in all the ways possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d69319c-7301-4f16-ad4a-88822b9fe74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [\n",
    "    'ноль', 'нуль',\n",
    "    'один', 'два', 'двe', 'три', 'четыре', 'пять', 'шесть', 'семь', 'восемь', 'девять', 'десять',\n",
    "    'одиннадцать', 'двенадцать', 'тринадцать', 'четырнадцать', 'пятнадцать', 'шестнадцать', 'семнадцать', 'восемнадцать', 'девятнадцать', 'двадцать',\n",
    "    'тридцать', 'сорок', 'пятьдесят', 'шестьдесят', 'семьдесят', 'восемьдесят', 'девяносто', 'сто',\n",
    "    'двести', 'триста', 'четыреста', 'пятьсот', 'шестьсот', 'семьсот', 'восемьсот', 'девятьсот',\n",
    "    'тысяча', 'миллион', 'миллиард', 'триллион',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4176d910-e2a1-45cb-b62d-cf2d3c1d8feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymorphy2 pymorphy2-dicts-ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281c4ede-b349-4fcc-9054-1a3d58b06219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "from itertools import chain\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def get_lexeme(word):\n",
    "    return set(chain(*([_.word for _ in parsing.lexeme] for parsing in morph.parse(word) if parsing.tag.POS in (\"NUMR\", \"NOUN\"))))\n",
    "\n",
    "get_lexeme(\"ноль\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1cb98d-a606-40f3-a3e3-2dcf0261c8b2",
   "metadata": {},
   "source": [
    "We face some mistakes as `семь` would be inflected as `семью` which is a form of `семья` as well so that we might want to do something about in in the future.\n",
    "We do an MVP now though so let it be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69874e0d-718c-4fee-bdae-85f3bc5fbe92",
   "metadata": {},
   "source": [
    "To not to search all the forms inflected one may to find a common part and change the (future) corresponding regexp according to it—and perform a fast `.contains()` check beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe660b-51b3-442d-8a5e-c07e09d05ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_common(words):\n",
    "    \"\"\"\n",
    "    Find a leading part only.\n",
    "\n",
    "    get_max_common([\"мама\", \"мать\", \"матриарх\"]) -> \"ма\"\n",
    "    \"\"\"\n",
    "    words = list(words)\n",
    "    if not words:\n",
    "        return None\n",
    "    result = words[0]\n",
    "    for word in words[1:]:\n",
    "        if word.startswith(result):\n",
    "            continue\n",
    "        for i, (ch1, ch2) in enumerate(zip(result, word)):\n",
    "            if ch1 != ch2:\n",
    "                result = result[:i]\n",
    "                break\n",
    "    return result\n",
    "\n",
    "get_max_common(get_lexeme(\"ноль\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c75598-23e5-4440-9bc5-09bf59a9eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "numbers_data = {}\n",
    "for number in numbers:\n",
    "    elem = {\n",
    "        \"word\": number,\n",
    "        \"lexeme\": get_lexeme(number)\n",
    "    }\n",
    "    elem[\"substr\"] = get_max_common(elem[\"lexeme\"])\n",
    "    elem[\"regexp\"] = re.compile(fr'\\b({elem[\"substr\"]}(?:{\"|\".join((_[len(elem[\"substr\"]):] for _ in elem[\"lexeme\"]))}))\\b')\n",
    "    numbers_data[number] = elem\n",
    "numbers_data[\"одиннадцать\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d494b04-ee12-4809-919c-16c43ce8686f",
   "metadata": {},
   "source": [
    "Now lets inspect what had we downloaded so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16efd176-889a-4c31-ba5a-103c9779a8f4",
   "metadata": {},
   "source": [
    "# pikabu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ef2d6-e387-4848-9191-61e87c91c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(load_dataset('/home/jovyan/wdc1/datasets/_WEB20/pikabu', split=\"train\", streaming=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfc66d5-2c66-471b-8982-a50998b84805",
   "metadata": {},
   "source": [
    "So we want to split texts as they are too big to fit into GPU as LLM train.\n",
    "\n",
    "We do not want to split on **sentences** now as the LLM we will train should see not single sentences only.\n",
    "One is not trivial to combine arbitrary sentences together.\n",
    "\n",
    "To split on paragraths (like `.split(\"\\n\")`) seems to be a good approach.\n",
    "\n",
    "We do not want to see latin and digits for now as we dont know how to normalize it so we filter any sentence containing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2b308-905a-4fdd-867a-642a7885a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "regexp_lat_dig = re.compile(r\"[a-zA-Z\\d]+\")\n",
    "\n",
    "\n",
    "def get_matches(text):\n",
    "    texts = text.split(\"\\n\")\n",
    "    result = []\n",
    "    for text in texts:\n",
    "        if re.search(regexp_lat_dig, text):\n",
    "            continue\n",
    "        matches = []\n",
    "        for number, elem in numbers_data.items():\n",
    "            if elem[\"substr\"] and elem[\"substr\"] not in text:\n",
    "                continue\n",
    "            if match := re.search(elem[\"regexp\"], text):\n",
    "                matches.append({\"number\": number, \"place\": match.span(), \"form\": match[0]})\n",
    "        if matches:\n",
    "            result.append({\n",
    "                \"text\": text,\n",
    "                \"matches\": matches\n",
    "            })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a306d-e23e-4be0-af99-4a1c7bff63fb",
   "metadata": {},
   "source": [
    "Now we are going to process a corpus and save the result into `jsonl` file now.\n",
    "\n",
    "I use multiprocessing as multiprocessing goes brrr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f20c97-f08b-445c-b50c-27a149c0661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/home/jovyan/wdc1/datasets/_WEB20/pikabu\"\n",
    "OUTPUT_PATH = \"pikabu.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a2be56-76e4-481f-ada5-dce60feba86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "queue = Queue()\n",
    "\n",
    "\n",
    "def process_example(**kwargs):\n",
    "    if matches := get_matches(kwargs[\"text_markdown\"]):\n",
    "        queue.put({\n",
    "            \"index\": kwargs[\"id\"],\n",
    "            \"texts\": matches\n",
    "        })\n",
    "\n",
    "\n",
    "def write(queue):\n",
    "    f = open(OUTPUT_PATH, \"w\")\n",
    "    while True:\n",
    "        item = queue.get()\n",
    "        if item is None:\n",
    "            return\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "writer = Process(target=write, args=(queue, ))\n",
    "writer.start()\n",
    "dataset = load_dataset(DATASET_PATH, split=\"train\", streaming=True)\n",
    "with Pool(10) as p:\n",
    "    for example in tqdm(dataset):\n",
    "        p.apply(process_example, kwds={**example})\n",
    "    queue.put(None)\n",
    "p.join()\n",
    "writer.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd352da-184d-46f8-8d67-b8ec574c0521",
   "metadata": {},
   "source": [
    "# librusec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da7a82-c7a7-4fc7-a9f5-214b78d693b9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Paragraths here are too big so we sentencize the texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f7ef5b-f66d-4a48-afb5-6435113093bd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Tried to use stanza but it turned out to be too slow.\n",
    "\n",
    "```\n",
    "!pip install stanza\n",
    "import stanza\n",
    "stanza.download('ru')\n",
    "nlp = stanza.Pipeline('ru', processors='tokenize')\n",
    "```\n",
    "\n",
    "Ended up with using spacy (haha classic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f99306-109d-47e6-b50a-05c1416a05d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a997e8e-aed5-4cb7-99fd-2622165e62c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "\n",
    "nlp_sentencizer = spacy.blank(\"ru\")\n",
    "nlp_sentencizer.add_pipe(\"sentencizer\")\n",
    "text = '\"В ходе проверочных мероприятий в целях профилактики правонарушений сотрудниками полиции было доставлено для административного разбирательства из центральной части города около 3 тысяч иностранных граждан. Как выяснилось, более 600 мигрантов находятся на территории России с различными нарушениями миграционного законодательства. Все они привлечены к административной ответственности\", - отметил собеседник агентства.'\n",
    "tokens = nlp_sentencizer(text)\n",
    "[str(sent) for sent in tokens.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539dbb31-b03c-423f-87ef-9f0e5e3b432a",
   "metadata": {},
   "source": [
    "Some boilerplate here.\n",
    "Have to design text parts separation externally—as a function, at least.\n",
    "\n",
    "Better to do a nice class but that depends on would I do that process again for some other corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c0a95-c08f-4ff3-bc9c-10e45917f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp_lat_dig = re.compile(r\"[a-zA-Z\\d]+\")\n",
    "\n",
    "\n",
    "def get_matches(text):\n",
    "    nlp_sentencizer.max_length = len(text) + 100\n",
    "    doc = nlp_sentencizer(text)\n",
    "    result = []\n",
    "    for sentence in enumerate(doc.sents):\n",
    "        print(str(sentence))\n",
    "        text = str(sentence)\n",
    "        if re.search(regexp_lat_dig, text):\n",
    "            continue\n",
    "        matches = []\n",
    "        for number, elem in numbers_data.items():\n",
    "            if elem[\"substr\"] and elem[\"substr\"] not in text:\n",
    "                continue\n",
    "            if match := re.search(elem[\"regexp\"], text):\n",
    "                matches.append({\"number\": number, \"place\": match.span(), \"form\": match[0]})\n",
    "        if matches:\n",
    "            result.append({\n",
    "                \"text\": text,\n",
    "                \"matches\": matches\n",
    "            })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af974bd-3d2d-49a8-b719-e06125bf4fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/home/jovyan/wdc1/datasets/_PLAIN/librusec\"\n",
    "OUTPUT_PATH = \"librusec.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c3b3ef-5155-4aa4-b91d-4e7479adcef6",
   "metadata": {},
   "source": [
    "Mostly the same but boilerplace again as the key is not `text_markdown` but `text` now.\n",
    "\n",
    "Should make some refactoring later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffbce50-d038-4506-b268-a86196e6447a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "queue = Queue()\n",
    "\n",
    "\n",
    "def process_example(**kwargs):\n",
    "    if matches := get_matches(kwargs[\"text\"]):\n",
    "        queue.put({\n",
    "            \"index\": kwargs[\"id\"],\n",
    "            \"texts\": matches\n",
    "        })\n",
    "\n",
    "\n",
    "def write(queue):\n",
    "    f = open(OUTPUT_PATH, \"w\")\n",
    "    while True:\n",
    "        item = queue.get()\n",
    "        if item is None:\n",
    "            return\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "writer = Process(target=write, args=(queue, ))\n",
    "writer.start()\n",
    "dataset = load_dataset(DATASET_PATH, split=\"train\", streaming=True)\n",
    "with Pool(10) as p:\n",
    "    for example in tqdm(dataset):\n",
    "        p.apply(process_example, kwds={**example})\n",
    "    queue.put(None)\n",
    "p.join()\n",
    "writer.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
