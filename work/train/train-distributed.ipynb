{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b1b475-1644-468a-afab-95306a4d69df",
   "metadata": {},
   "source": [
    "It is the time to thain something finally.\n",
    "\n",
    "Based on [translation.ipynb](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/translation.ipynb) and [fred-t5 finetune repo](https://github.com/Den4ikAI/FRED-T5-Finetuning). Modified as in [`tensor_parallel` example](https://github.com/BlackSamorez/tensor_parallel/blob/main/examples/training_flan-t5-xl.ipynb).\n",
    "\n",
    "~~I use two of my 4x RTX3060 12GB rig as use of 3+ GPUs cause `Bus error (core dumped)` error. One is necessary to restart the jupyterlab docker container then in order to recover it.~~ Fixed it by [increasing](https://github.com/pytorch/pytorch/issues/2244) shared memoty container size.\n",
    "\n",
    "> It is possible to use `\"cuda:3\"` device for a single gpu but `\"cuda:2,3\"` seems to be not supported by ü§ó thansformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db03144e-02ba-427d-bb11-b2f9e759437e",
   "metadata": {},
   "source": [
    "`tensor_parallel` does not work with modern versions of transformers (despite its official requirements) so I had to downgrade it manually.\n",
    "```\n",
    "!pip install tensor_parallel\n",
    "!pip install transformers==4.29.2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf75a0-c419-447a-ae82-7d4d38821c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3edf4a5-488c-494c-b520-acd70b9028e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b44495-e1b9-445a-b1c5-81436dee6820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install datasets transformers[torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c47bb-4eba-473b-b609-600e2b54bc61",
   "metadata": {},
   "source": [
    "I use a part of the data I have as the model trains too long otherwise.\n",
    "8-12 hours of finetuning was just fine for my usual task so I prefer to hold on to this here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce617a23-6133-47e7-93fe-7e2f74c943d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = [\n",
    "    \"/home/jovyan/data/kaggle.jsonl\",\n",
    "    \"/home/jovyan/data/ficbook_replaces.jsonl\",\n",
    "    \"/home/jovyan/data/pikabu_replaces.jsonl\",\n",
    "    # \"/home/jovyan/data/librusec_replaces.jsonl\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca096ee-0ec4-4b29-b4d3-c3a8e21db0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = \"/home/jovyan/wdc1/models/FRED-T5-1.7B\"\n",
    "# MODEL_PATH = \"/home/jovyan/wdc1/models/FRED-T5-large\"\n",
    "MODEL_PATH = \"/home/jovyan/wdc1/models/ruT5-base\"\n",
    "# MODEL_PATH = \"/home/jovyan/models/3_fred-t5/checkpoint-11000\"\n",
    "\n",
    "TRAINED_SAVE_PATH = \"/home/jovyan/models/8_ruT5-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30e7f0b-c5c3-49d9-83c3-51874e5d61ca",
   "metadata": {},
   "source": [
    "In case of `ruT5-base` training do\n",
    "\n",
    "```python\n",
    "!pip install datasets transformers[sentencepiece]\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "path = \"./ruT5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(path)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30a2cb-5b29-42c9-9153-25f97f693b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38bd9ac-f71d-4592-a6f2-fb8d50932f93",
   "metadata": {},
   "source": [
    "# He obtayn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dcf538-7d11-49f4-a98b-d7732d9bb0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for file in FILES:\n",
    "    with open(file) as f:\n",
    "        for line in tqdm(f, desc=file):\n",
    "            dataset.append({\"replaces\": json.loads(line)[\"replaces\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68172e55-55b3-4288-9404-646a5a730c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b61374b-eb2c-489e-91e2-2c3a1ece0a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, T5Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH)\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(MODEL_PATH, eos_token='</s>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3677846-72b3-47c2-b92c-a1a7a146006b",
   "metadata": {},
   "source": [
    "One problem about the last train iteration was deluded prediction of long numbers like `125678`.\n",
    "It could possibly happen because of tokenization of numbers if divided on parts which are not easy to operate.\n",
    "Lets check it out now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a2e95-d83d-4ea4-93e4-af39707368c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "good, wrong = [], []\n",
    "for i in range(100, 1000):\n",
    "    a = str(i)\n",
    "    ids = tokenizer.encode(a, add_special_tokens=False)\n",
    "    b = \"|\".join([tokenizer.decode(_, skip_special_tokens=True) for _ in ids])\n",
    "    (wrong if a != b else good).append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03ffd0-915c-43cf-ae1a-719d2f7287f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good), len(wrong)\n",
    "# (28, 872)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c25ee62-2672-4b6b-862f-de792fa2063f",
   "metadata": {},
   "source": [
    "The particular `FRED-T5-large` tokenizer splitted the majority of the three digits numbers.\n",
    "May be it would be better if numbers are forced splitted on single digits like `123456` to `1 2 3 4 5 6`.\n",
    "\n",
    "Other option is to divide numbers by three digit groups such that `1234567` would turn into `1 234 567`. We try that option first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d00c8d-463b-46f3-a447-c5a26036885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_numbers(s):\n",
    "    return \" \".join(((\" \".join(part) if part.isdigit() else part) for part in s.split()))\n",
    "\n",
    "\n",
    "def strip_numbers(s):\n",
    "    result = []\n",
    "    for part in s.split():\n",
    "        if part.isdigit():\n",
    "            while len(part) > 3:\n",
    "                result.append(part[:- 3 * ((len(part) - 1) // 3)])\n",
    "                part = part[- 3 * ((len(part) - 1) // 3):]\n",
    "            if part:\n",
    "                result.append(part)\n",
    "        else:\n",
    "            result.append(part)\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "strip_numbers(\"—É –Ω–∞—Å –±—ã–ª–æ 1234567890 –ø–∞–∫–µ—Ç–∏–∫–æ–≤ —Ç—Ä–∞–≤—ã, 750 –∞–º–ø—É–ª –Ω–æ–≤–æ–∫–∞–∏–Ω–∞, 55555 –ø–∞–∫–µ—Ç–∏–∫–æ–≤ –¥–∏—ç—Ç–∏–ª–∞–º–∏–¥–∞ –ª–∏–∑–µ—Ä–≥–∏–Ω–æ–≤–æ–π –∫–∏—Å–ª–æ—Ç—ã, —Å–æ–ª–æ–Ω–∫–∞, –Ω–∞ 1000/2000 –Ω–∞–ø–æ–ª–Ω–µ–Ω–Ω–∞—è –∫–æ–∫–∞–∏–Ω–æ–º\")\n",
    "# \"—É –Ω–∞—Å –±—ã–ª–æ 1 234 567 890 –ø–∞–∫–µ—Ç–∏–∫–æ–≤ —Ç—Ä–∞–≤—ã, 750 –∞–º–ø—É–ª –Ω–æ–≤–æ–∫–∞–∏–Ω–∞, 55 555 –ø–∞–∫–µ—Ç–∏–∫–æ–≤ –¥–∏—ç—Ç–∏–ª–∞–º–∏–¥–∞ –ª–∏–∑–µ—Ä–≥–∏–Ω–æ–≤–æ–π –∫–∏—Å–ª–æ—Ç—ã, —Å–æ–ª–æ–Ω–∫–∞, –Ω–∞ 1000/2000 –Ω–∞–ø–æ–ª–Ω–µ–Ω–Ω–∞—è –∫–æ–∫–∞–∏–Ω–æ–º\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab6a775-95df-4077-a81a-d43bb231b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd())))\n",
    "from replaces import Replace, Replaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb3d93-4339-4f06-b41c-5a37ea0dd5f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "data = []\n",
    "# occ_limit = len(dataset) / 100  # rough trim here\n",
    "added = Counter()\n",
    "for elem in tqdm(dataset):\n",
    "    elem[\"replaces\"] = Replaces(elem[\"replaces\"])  # recover class stuff\n",
    "    if all(_.type == \"E\" for _ in elem[\"replaces\"]):\n",
    "        continue\n",
    "    if \"prompt\" in elem and \"target\" in elem:\n",
    "        continue\n",
    "    replace_words = list(chain(*(r.text_to.strip().lower().split() for r in elem[\"replaces\"] if r.type != \"E\")))\n",
    "    # if not any(added[word] < occ_limit for word in replace_words):\n",
    "    #     continue\n",
    "    added.update(replace_words)\n",
    "    prompt, target = \"<SC1>\", \"\"\n",
    "    etid = 0\n",
    "    for r in elem[\"replaces\"]:\n",
    "        if r.type == \"E\":\n",
    "            prompt += r.text_to\n",
    "        else:\n",
    "            ws_number = len(r.text_from) - len(r.text_from.rstrip())\n",
    "            prompt += f\"[{strip_numbers(r.text_from.rstrip())}]<extra_id_{etid}>{' ' * ws_number}\"\n",
    "            target += f\"<extra_id_{etid}> {r.text_to.strip()}\\n\"\n",
    "            etid += 1\n",
    "    elem[\"prompt\"] = f\"{prompt}</s>\"\n",
    "    elem[\"target\"] = f\"{target}</s>\"\n",
    "    data.append(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e309d4-d538-40a3-af7e-9e2bb1202850",
   "metadata": {},
   "source": [
    "We made here train examples of that kind\n",
    "\n",
    "    <SC1>–í—Ä–µ–º–µ–Ω–∞–º–∏ —è –¥—É–º–∞—é, –∫–∞–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–∞–π—Ç–∏ —Ç–µ–º [14 697]<extra_id_0> —Ä—É–±–ª—è–º, —á—Ç–æ –ª–µ–∂–∞—Ç —É–∂–µ –±–æ–ª—å—à–µ [33]<extra_id_1> –ª–µ—Ç?\n",
    "\n",
    "and we want to predict a text like this\n",
    "\n",
    "    <extra_id_0> —á–µ—Ç—ã—Ä–Ω–∞–¥—Ü–∞—Ç–∏ —Ç—ã—Å—è—á–∞–º —à–µ—Å—Ç–∏—Å—Ç–∞–º –¥–µ–≤—è–Ω–æ—Å—Ç–∞ —Å–µ–º–∏\n",
    "    <extra_id_1> —Ç—Ä–∏–¥—Ü–∞—Ç–∏ —Ç—Ä—ë—Ö </s>\n",
    "\n",
    "Lets check what have we added so far like the most (un)common __words__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde1073-7c33-4537-b0cb-587ed17de332",
   "metadata": {},
   "outputs": [],
   "source": [
    "added.most_common()[:10], added.most_common()[-10:], data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadf4e99-c7ca-46d8-b86c-8ee8a67a5938",
   "metadata": {},
   "source": [
    "Besides rare mistakes it seems to be trainable on.\n",
    "\n",
    "The distribution is shifted anyway to my taste as will be shown later.\n",
    "One fast and simple thing to do about it is to iterate over and filter examples as we have seen too much of **all** the replaced words at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851a6301-7160-421d-bc04-174abfc3a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_limit = (sum(added.values()) / len(added)) ** 2\n",
    "print(occ_limit)\n",
    "added2 = Counter()\n",
    "balanced_data = []\n",
    "for elem in tqdm(data):\n",
    "    replace_words = list(chain(*[r.text_to.strip().lower().split() for r in elem[\"replaces\"] if r.type != \"E\"]))\n",
    "    if any((added2[word] < occ_limit for word in replace_words)):\n",
    "        balanced_data.append(elem)\n",
    "        added2.update(replace_words)\n",
    "len(balanced_data), len(balanced_data) / len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b9beb-a30e-4b3a-8014-dde28be310f0",
   "metadata": {},
   "source": [
    "We have gotten rid of 2/3 of the data we had had!\n",
    "Check it out visually now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe982700-1d5c-458c-9a60-4ce1f85889ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_regs = {\n",
    "    \"re_digits\": re.compile(r\"\\d\"),\n",
    "    \"re_digits_latin\": re.compile(r\"[a-zA-Z\\d]\"),\n",
    "    \"re_latin\": re.compile(r\"[a-zA-Z]\")\n",
    "}\n",
    "for stat_name, stat_re in stat_regs.items():\n",
    "    print(\n",
    "        stat_name,\n",
    "        len([elem for elem in tqdm(balanced_data) if any(re.search(stat_re, r.text_from) for r in elem[\"replaces\"])])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52f0fe3-b7b0-444c-b124-d3e7a9a86257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "axs = plt.subplot()\n",
    "axs.set_yscale('log')\n",
    "axs.plot([_[1] for _ in added2.most_common()[:1000]])\n",
    "axs.plot([_[1] for _ in added.most_common()[:1000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e31a12-47a8-4273-9d34-e50074d5b8fe",
   "metadata": {},
   "source": [
    "Only extra data lost so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df676f67-1c4d-441d-9072-c30d44c9c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb88cae-2d41-4d23-989f-15fd8afc6ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "dataset = Dataset.from_list(balanced_data).train_test_split(test_size=0.01)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a8d28-64e2-4758-8b7f-e930151d2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"prompt\"],\n",
    "        text_target=examples[\"target\"],\n",
    "        max_length=128,  # NB should affect memory consumption\n",
    "        truncation=True\n",
    "    )\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "dataset = dataset.map(preprocess_function, batched=True, num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9a47b-1042-4334-9692-2af334eb1265",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([\"prompt\", \"target\", \"replaces\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0abc0d-8a6e-4ff7-a45b-2da3151548c5",
   "metadata": {},
   "source": [
    "Just in case I get rid of examples with possible truncation mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66e513-dfa3-4f93-b798-6738dae70584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter([len(_[\"input_ids\"]) for _ in dataset[\"train\"]])\n",
    "sum([v for k, v in c.items() if k < 128]), c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6917cfec-1674-4658-aac6-c4f03df8705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in dataset.items():\n",
    "    dataset[k] = [_ for _ in v if 10 < len(_[\"input_ids\"]) < 126]\n",
    "{k:len(v) for k, v in dataset.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669201f4-ad10-4c6b-8eed-14e712d3683a",
   "metadata": {},
   "source": [
    "# He trayn\n",
    "\n",
    "Time to train actually as last!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c31ca-1755-4033-a45b-f2443d14ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    offload_state_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949871ea-8c32-4304-9c8f-36716766590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensor_parallel as tp\n",
    "\n",
    "\n",
    "model = tp.tensor_parallel(\n",
    "    model,\n",
    "    [\"cuda:0\", \"cuda:1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23105760-1e79-4641-8d75-e017719ef647",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"A cat sat on a mat\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "output_ids = tokenizer(\"A cat sat did not sit on a mat\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "loss = model(input_ids=input_ids, labels=output_ids).loss\n",
    "loss.backward()  # check nvidia-smi for gpu memory usage :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ab02c-e7a6-42c1-8274-eca363083ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12cc228-f9db-441e-b406-62c7dfed0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer,  DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=TRAINED_SAVE_PATH,\n",
    "    # optim=\"adamw_bnb_8bit\",\n",
    "    optim=\"adafactor\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    logging_first_step=True,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    # gradient_checkpointing=True,\n",
    "    gradient_checkpointing=False,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    save_total_limit=10,\n",
    "    num_train_epochs=2,\n",
    "    # predict_with_generate=True,\n",
    "    # fp16=True,\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    # auto_find_batch_size=True,\n",
    "    auto_find_batch_size=False,\n",
    "    dataloader_num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b445baa-b12a-48f1-9dba-2601b8175c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4642ba79-cca3-4ddc-8b5e-6255931af701",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tp.save_tensor_parallel(model):\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
    "        # compute_metrics=compute_metrics,\n",
    "        # optimizers=(adam_bnb_optim, None),\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e2fe2b-1a58-48c3-a7be-37675fb723c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tp.save_tensor_parallel(model):\n",
    "    model.save_pretrained(os.path.join(TRAINED_SAVE_PATH, \"final\"))\n",
    "    tokenizer.save_pretrained(os.path.join(TRAINED_SAVE_PATH, \"final\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b447b82f-5214-41b7-85ec-2e23271ddce6",
   "metadata": {},
   "source": [
    "# But most importantly he explayn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a1834-3688-47f2-9519-2158f9c930e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "lm_text = '<SC1>—è –∫—É–ø–∏–ª [iphone 12X]<extra_id_0> –∑–∞ [142 990]<extra_id_1> —Ä—É–± –±–µ–∑ [3-x]<extra_id_2> —á–∞—Å–æ–≤ –ø–æ–ª–¥–µ–Ω—å –∏ —Ç.–¥.'\n",
    "# lm_text = '<SC1>—è –∫—É–ø–∏–ª –∞–π—Ñ–æ–Ω –∑–∞ [14 970]<extra_id_0> —Ä—É–±–ª–µ–π'\n",
    "lm_text = \"<SC1>–í—Ä–µ–º–µ–Ω–∞–º–∏ —è –¥—É–º–∞—é, –∫–∞–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–∞–π—Ç–∏ —Ç–µ–º [14 697]<extra_id_0> —Ä—É–±–ª—è–º, —á—Ç–æ –ª–µ–∂–∞—Ç —É–∂–µ –±–æ–ª—å—à–µ [33]<extra_id_1> –ª–µ—Ç?\"\n",
    "# lm_text = \"<SC1>–ë—ã–ª–æ —É –æ—Ç—Ü–∞ [3]<extra_id_0> —Å—ã–Ω–∞, –Ω–æ –Ω–µ –±—ã–ª–æ –¥–∞–∂–µ [2-3]<extra_id_1> –ø–∏–¥–∂–∞–∫–æ–≤ —Å –±–ª—ë—Å—Ç–∫–∞–º–∏ –∑–∞ [142 990 —Ä—É–±]<extra_id_2>.\"\n",
    "# lm_text = \"<SC1>–í —à–∫–æ–ª–µ —É –º–µ–Ω—è –æ–¥–Ω–∏ [5]<extra_id_0>.\"\n",
    "# lm_text = '<SC1>–ë—ã–ª–æ —É –æ—Ç—Ü–∞ [3]<extra_id_0> —Å—ã–Ω–∞. –°—Ç–∞—Ä—à–µ–º—É –±—ã–ª–æ [35]<extra_id_1>, —Å—Ä–µ–¥–Ω–µ–º—É - –Ω–µ –º–µ–Ω—å—à–µ [33]<extra_id_2>, –∞ –º–ª–∞–¥—à–∏–π –Ω–∞ [4]<extra_id_3> –º–ª–∞–¥—à–µ –≤—Å–µ—Ö. –ë—ã–≤–∞–µ—Ç.'\n",
    "lm_text = \"<SC1>–í—Ä–µ–º–µ–Ω–∞–º–∏ —è –¥—É–º–∞—é, –∫–∞–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–∞–π—Ç–∏ —Ç–µ–º [265 948 697]<extra_id_0> —Ä—É–±–ª—è–º, —á—Ç–æ –ª–µ–∂–∞—Ç —É–∂–µ –±–æ–ª—å—à–µ [33]<extra_id_1> –ª–µ—Ç?\"\n",
    "input_ids = torch.tensor([tokenizer.encode(lm_text)]).to(\"cuda:0\")\n",
    "outputs = model.generate(input_ids, eos_token_id=tokenizer.eos_token_id, early_stopping=True)\n",
    "print(tokenizer.decode(outputs[0][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d7a17-ee66-4e45-844b-07b7cae253b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
