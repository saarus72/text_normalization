{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b1b475-1644-468a-afab-95306a4d69df",
   "metadata": {},
   "source": [
    "It is the time to thain something finally.\n",
    "\n",
    "Based on [translation.ipynb](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/translation.ipynb) and [fred-t5 finetune repo](https://github.com/Den4ikAI/FRED-T5-Finetuning). Modified as in [`tensor_parallel` example](https://github.com/BlackSamorez/tensor_parallel/blob/main/examples/training_flan-t5-xl.ipynb).\n",
    "\n",
    "I use two of my 4x RTX3060 12GB rig as use of 3+ GPUs cause `Bus error (core dumped)` error. One is necessary to restart the jupyterlab docker container then in order to recover it.\n",
    "\n",
    "> It is possible to use `\"cuda:3\"` device for a single gpu but `\"cuda:2,3\"` seems to be not supported by ü§ó thansformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db03144e-02ba-427d-bb11-b2f9e759437e",
   "metadata": {},
   "source": [
    "`tensor_parallel` does not work with modern versions of transformers (despite its official requirements) so I had to downgrade it manually.\n",
    "```\n",
    "!pip install tensor_parallel\n",
    "!pip install transformers==4.29.2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf75a0-c419-447a-ae82-7d4d38821c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3edf4a5-488c-494c-b520-acd70b9028e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b44495-e1b9-445a-b1c5-81436dee6820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install datasets transformers[torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c47bb-4eba-473b-b609-600e2b54bc61",
   "metadata": {},
   "source": [
    "I use a part of the data I have as the model trains too long otherwise.\n",
    "8-12 hours of finetuning was just fine for my usual task so I prefer to hold on to this here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce617a23-6133-47e7-93fe-7e2f74c943d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = [\n",
    "    \"/home/jovyan/work/dataset/ficbook_pairs.jsonl\",\n",
    "    # # should be first as its index fields are str not integers\n",
    "    # # otherwise field type can be specified explicitely like\n",
    "    # # `features=Features({'prompt': Value('string'), 'target': Value('string')})`\n",
    "    \"/home/jovyan/work/dataset/pikabu_pairs.jsonl\",\n",
    "    # \"/home/jovyan/work/dataset/librusec_pairs.jsonl\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca096ee-0ec4-4b29-b4d3-c3a8e21db0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/home/jovyan/wdc1/models/FRED-T5-1.7B\"\n",
    "# MODEL_PATH = \"/home/jovyan/wdc1/models/FRED-T5-large\"\n",
    "TRAINED_SAVE_PATH = \"/home/jovyan/work/models/2_fred-t5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dcf538-7d11-49f4-a98b-d7732d9bb0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Features, Value, Dataset\n",
    "\n",
    "dataset = load_dataset('json', data_files=FILES)\n",
    "# dataset = dataset['train']#.train_test_split(test_size=0.1)\n",
    "dataset, dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38bd9ac-f71d-4592-a6f2-fb8d50932f93",
   "metadata": {},
   "source": [
    "# He obtayn\n",
    "\n",
    "I decided to put a construction of train examples alongside the training code itself as\n",
    "* it is fast actually and\n",
    "* I do see the preprocessing as a part of the future model.\n",
    "\n",
    "So, here is the code.\n",
    "It finds parts of two lines which are different and construct that \"before\" and \"after\" thing.\n",
    "It filters identical pairs as well since there is nothing to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfefd134-f4d2-4177-a458-90f31ba0a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "class Replace(dict):\n",
    "    def __init__(\n",
    "        self,\n",
    "        type: str, text_from: str, text_to: Optional[str]=None,\n",
    "        *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self[\"type\"] = type\n",
    "        self[\"text_from\"] = text_from\n",
    "        self[\"text_to\"] = \"\" if not text_to else text_to\n",
    "\n",
    "    @property\n",
    "    def type(self):\n",
    "        return self[\"type\"]\n",
    "\n",
    "    @property\n",
    "    def text_from(self):\n",
    "        return self[\"text_from\"]\n",
    "\n",
    "    @property\n",
    "    def text_to(self):\n",
    "        return self[\"text_to\"]\n",
    "\n",
    "    def extend(self, r):\n",
    "        if self.type != r.type:\n",
    "            raise Exception(\"Replace type mismatch\")\n",
    "        self[\"text_from\"] += r[\"text_from\"]\n",
    "        self[\"text_to\"] += r[\"text_to\"]\n",
    "\n",
    "\n",
    "class Replaces(list):\n",
    "    def add(self, r: Replace):\n",
    "        if self and r.type == self[-1].type:\n",
    "            self[-1].extend(r)\n",
    "        else:\n",
    "            return super().append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af95db67-c38a-4b33-af73-e48087dd7580",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tokens = re.compile(r\"[–∞-—è–ê-–Ø]+\\s*|\\d+(?:\\.\\d+)?\\s*|[^–∞-—è–ê-–Ø\\d\\s]+\\s*\")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(re_tokens, text)\n",
    "\n",
    "\n",
    "\"|\".join(tokenize(\"—Ç—ã, –¥–∞ —è, –¥–∞ –º—ã c —Ç–æ–±–æ–π - –≤–º–µ—Å—Ç–µ 2.5.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c20bf-ba51-4866-8d05-28984a9562ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_digits = re.compile(r\"\\d\")\n",
    "\n",
    "\n",
    "def diff(seq1, seq2):\n",
    "    sm = SequenceMatcher(\n",
    "        # lambda x: not re.search(r\"\\w\", x.strip()),\n",
    "        a=seq1,\n",
    "        b=seq2,\n",
    "        autojunk=False\n",
    "    )\n",
    "    result = Replaces()\n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        # print(tag, \" \".join(seq1[i1:i2]), \" \".join(seq2[j1:j2]))\n",
    "        text_from, text_to = \"\".join(seq1[i1:i2]), \"\".join(seq2[j1:j2])\n",
    "        if tag == \"equal\":\n",
    "            type = \"E\"\n",
    "        elif tag == \"replace\" and \"\".join((_.strip() for _ in seq1[i1:i2])) == \"\".join((_.strip() for _ in seq2[j1:j2])):\n",
    "            type = \"E\"\n",
    "        else:\n",
    "            if not re.search(re_digits, text_from) and not re.search(re_digits, text_to):\n",
    "                type = \"E\"\n",
    "                text_to = None\n",
    "            else:\n",
    "                type = \"R\"\n",
    "        result.add(Replace(type, text_from, text_to))\n",
    "    return result\n",
    "\n",
    "\n",
    "diff(\n",
    "    tokenize(\"–í—Å–µ–≥–æ - —Ç—Ä–∏–¥—Ü–∞—Ç—å –¥–≤–∞ —Å –ø–æ–ª–æ–≤–∏–Ω–æ–π.\"),\n",
    "    tokenize(\"–í—Å–µ–≥–æ - 32.5.\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68172e55-55b3-4288-9404-646a5a730c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b61374b-eb2c-489e-91e2-2c3a1ece0a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_PATH, eos_token='</s>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3677846-72b3-47c2-b92c-a1a7a146006b",
   "metadata": {},
   "source": [
    "One problem about the last train iteration was deluded prediction of long numbers like `125678`.\n",
    "It could possibly happen because of tokenization of numbers if divided on parts which are not easy to operate.\n",
    "Lets check it out now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a2e95-d83d-4ea4-93e4-af39707368c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "good, wrong = [], []\n",
    "for i in range(100, 1000):\n",
    "    a = str(i)\n",
    "    ids = tokenizer.encode(a)\n",
    "    b = \"|\".join([tokenizer.decode(_) for _ in ids])\n",
    "    (wrong if a != b else good).append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03ffd0-915c-43cf-ae1a-719d2f7287f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good), len(wrong)\n",
    "# (28, 872)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c25ee62-2672-4b6b-862f-de792fa2063f",
   "metadata": {},
   "source": [
    "The particular `FRED-T5-large` tokenizer splitted the majority of the three digits numbers.\n",
    "May be it would be better if numbers are forced splitted on single digits like `123456` to `1 2 3 4 5 6`.\n",
    "\n",
    "Other option is to divide numbers by three digit groups such that `1234567` would turn into `1 234 567`. We try that option first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d00c8d-463b-46f3-a447-c5a26036885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_numbers(s):\n",
    "    return \" \".join(((\" \".join(part) if part.isdigit() else part) for part in s.split()))\n",
    "\n",
    "\n",
    "def strip_numbers(s):\n",
    "    result = []\n",
    "    for part in s.split():\n",
    "        if part.isdigit():\n",
    "            while len(part) > 3:\n",
    "                result.append(part[:- 3 * ((len(part) - 1) // 3)])\n",
    "                part = part[- 3 * ((len(part) - 1) // 3):]\n",
    "            if part:\n",
    "                result.append(part)\n",
    "        else:\n",
    "            result.append(part)\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "strip_numbers(\"—É –Ω–∞—Å –±—ã–ª–æ 1234567890 –ø–∞–∫–µ—Ç–∏–∫–æ–≤ —Ç—Ä–∞–≤—ã, 750 –∞–º–ø—É–ª –Ω–æ–≤–æ–∫–∞–∏–Ω–∞, 55555 –ø–∞–∫–µ—Ç–∏–∫–æ–≤ –¥–∏—ç—Ç–∏–ª–∞–º–∏–¥–∞ –ª–∏–∑–µ—Ä–≥–∏–Ω–æ–≤–æ–π –∫–∏—Å–ª–æ—Ç—ã, —Å–æ–ª–æ–Ω–∫–∞, –Ω–∞ 1000/2000 –Ω–∞–ø–æ–ª–Ω–µ–Ω–Ω–∞—è –∫–æ–∫–∞–∏–Ω–æ–º\")\n",
    "# \"—É –Ω–∞—Å –±—ã–ª–æ 1 234 567 890 –ø–∞–∫–µ—Ç–∏–∫–æ–≤ —Ç—Ä–∞–≤—ã, 750 –∞–º–ø—É–ª –Ω–æ–≤–æ–∫–∞–∏–Ω–∞, 55 555 –ø–∞–∫–µ—Ç–∏–∫–æ–≤ –¥–∏—ç—Ç–∏–ª–∞–º–∏–¥–∞ –ª–∏–∑–µ—Ä–≥–∏–Ω–æ–≤–æ–π –∫–∏—Å–ª–æ—Ç—ã, —Å–æ–ª–æ–Ω–∫–∞, –Ω–∞ 1000/2000 –Ω–∞–ø–æ–ª–Ω–µ–Ω–Ω–∞—è –∫–æ–∫–∞–∏–Ω–æ–º\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb3d93-4339-4f06-b41c-5a37ea0dd5f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "data = []\n",
    "occ_limit = len(dataset[\"train\"]) / 100  # rough trim here\n",
    "added = Counter()\n",
    "for elem in tqdm(dataset[\"train\"]):\n",
    "    elem.setdefault(\"replaces\", diff(tokenize(elem[\"tn\"]), tokenize(elem[\"itn\"])))\n",
    "    if all(_.type == \"E\" for _ in elem[\"replaces\"]):\n",
    "        continue\n",
    "    if \"prompt\" in elem and \"target\" in elem:\n",
    "        continue\n",
    "    replace_words = list(chain(*(r.text_from.strip().lower().split() for r in elem[\"replaces\"] if r.type != \"E\")))\n",
    "    if not any(added[word] < occ_limit for word in replace_words):\n",
    "        continue\n",
    "    added.update(replace_words)\n",
    "    prompt, target = \"<SC1>\", \"\"\n",
    "    etid = 0\n",
    "    for r in elem[\"replaces\"]:\n",
    "        if r.type == \"E\":\n",
    "            prompt += r.text_from\n",
    "        else:\n",
    "            ws_number = len(r.text_to) - len(r.text_to.rstrip())\n",
    "            prompt += f\"[{strip_numbers(r.text_to.rstrip())}]<extra_id_{etid}>{' ' * ws_number}\"\n",
    "            target += f\"<extra_id_{etid}> {r.text_from.strip()}\\n\"\n",
    "            etid += 1\n",
    "    elem[\"prompt\"] = f\"{prompt}</s>\"\n",
    "    elem[\"target\"] = f\"{target}</s>\"\n",
    "    data.append(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e309d4-d538-40a3-af7e-9e2bb1202850",
   "metadata": {},
   "source": [
    "We made here train examples of that kind\n",
    "\n",
    "    <SC1>–í—Ä–µ–º–µ–Ω–∞–º–∏ —è –¥—É–º–∞—é, –∫–∞–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–∞–π—Ç–∏ —Ç–µ–º [14 697]<extra_id_0> —Ä—É–±–ª—è–º, —á—Ç–æ –ª–µ–∂–∞—Ç —É–∂–µ –±–æ–ª—å—à–µ [33]<extra_id_1> –ª–µ—Ç?\n",
    "\n",
    "and we want to predict a text like this\n",
    "\n",
    "    <extra_id_0> —á–µ—Ç—ã—Ä–Ω–∞–¥—Ü–∞—Ç–∏ —Ç—ã—Å—è—á–∞–º —à–µ—Å—Ç–∏—Å—Ç–∞–º –¥–µ–≤—è–Ω–æ—Å—Ç–∞ —Å–µ–º–∏\n",
    "    <extra_id_1> —Ç—Ä–∏–¥—Ü–∞—Ç–∏ —Ç—Ä—ë—Ö </s>\n",
    "\n",
    "Lets check what have we added so far like the most (un)common __words__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde1073-7c33-4537-b0cb-587ed17de332",
   "metadata": {},
   "outputs": [],
   "source": [
    "added.most_common()[:10], added.most_common()[-10:], data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadf4e99-c7ca-46d8-b86c-8ee8a67a5938",
   "metadata": {},
   "source": [
    "Besides rare mistakes it seems to be trainable on.\n",
    "\n",
    "The distribution is shifted anyway to my taste as will be shown later.\n",
    "One fast and simple thing to do about it is to iterate over and filter examples as we have seen too much of **all** the replaced words at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851a6301-7160-421d-bc04-174abfc3a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_limit = sum(added.values()) / len(added)\n",
    "print(occ_limit)\n",
    "added2 = Counter()\n",
    "balanced_data = []\n",
    "for elem in tqdm(data):\n",
    "    replace_words = list(chain(*[r.text_from.strip().lower().split() for r in elem[\"replaces\"] if r.type != \"E\"]))\n",
    "    if any((added2[word] < occ_limit for word in replace_words)):\n",
    "        balanced_data.append(elem)\n",
    "        added2.update(replace_words)\n",
    "len(balanced_data) / len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b9beb-a30e-4b3a-8014-dde28be310f0",
   "metadata": {},
   "source": [
    "We have gotten rid of 2/3 of the data we had had!\n",
    "Check it out visually now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52f0fe3-b7b0-444c-b124-d3e7a9a86257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "axs = plt.subplot()\n",
    "axs.set_yscale('log')\n",
    "x = [1, 2, 3, 4, 5] \n",
    "y = [1, 4, 9, 16, 25] \n",
    "axs.plot([_[1] for _ in added2.most_common()])\n",
    "axs.plot([_[1] for _ in added.most_common()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e31a12-47a8-4273-9d34-e50074d5b8fe",
   "metadata": {},
   "source": [
    "Only extra data lost so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df676f67-1c4d-441d-9072-c30d44c9c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb88cae-2d41-4d23-989f-15fd8afc6ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list([_ for _ in balanced_data]).train_test_split(test_size=0.02)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a8d28-64e2-4758-8b7f-e930151d2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"prompt\"],\n",
    "        text_target=examples[\"target\"],\n",
    "        max_length=128,  # NB should affect memory consumption\n",
    "        truncation=True\n",
    "    )\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "dataset = dataset.map(preprocess_function, batched=True, num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9a47b-1042-4334-9692-2af334eb1265",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([\"prompt\", \"target\", \"tn\", \"itn\", \"orig_index\", \"text_index\", \"replaces\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0abc0d-8a6e-4ff7-a45b-2da3151548c5",
   "metadata": {},
   "source": [
    "Just in case I get rid of examples with possible truncation mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66e513-dfa3-4f93-b798-6738dae70584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter([len(_[\"input_ids\"]) for _ in dataset[\"train\"]])\n",
    "sum([v for k, v in c.items() if k < 128]), c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6917cfec-1674-4658-aac6-c4f03df8705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in dataset.items():\n",
    "    dataset[k] = [_ for _ in v if 10 < len(_[\"input_ids\"]) < 126]\n",
    "{k:len(v) for k, v in dataset.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669201f4-ad10-4c6b-8eed-14e712d3683a",
   "metadata": {},
   "source": [
    "# He trayn\n",
    "\n",
    "Time to train actually as last!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c31ca-1755-4033-a45b-f2443d14ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    offload_state_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30e7f0b-c5c3-49d9-83c3-51874e5d61ca",
   "metadata": {},
   "source": [
    "In case of `ruT5-base` training use these lines:\n",
    "\n",
    "```python\n",
    "!pip install datasets transformers[sentencepiece]\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "path = \"./ruT5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(path)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949871ea-8c32-4304-9c8f-36716766590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensor_parallel as tp\n",
    "\n",
    "\n",
    "model = tp.tensor_parallel(\n",
    "    model,\n",
    "    [\"cuda:0\", \"cuda:1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23105760-1e79-4641-8d75-e017719ef647",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"A cat sat on a mat\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "output_ids = tokenizer(\"A cat sat did not sit on a mat\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "loss = model(input_ids=input_ids, labels=output_ids).loss\n",
    "loss.backward()  # check nvidia-smi for gpu memory usage :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ab02c-e7a6-42c1-8274-eca363083ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12cc228-f9db-441e-b406-62c7dfed0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer,  DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=TRAINED_SAVE_PATH,\n",
    "    # optim=\"adamw_bnb_8bit\",\n",
    "    optim=\"adafactor\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    logging_first_step=True,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    # gradient_checkpointing=True,\n",
    "    gradient_checkpointing=False,\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_total_limit=20,\n",
    "    num_train_epochs=2,\n",
    "    # predict_with_generate=True,\n",
    "    # fp16=True,\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    # auto_find_batch_size=True,\n",
    "    auto_find_batch_size=False,\n",
    "    dataloader_num_workers=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b445baa-b12a-48f1-9dba-2601b8175c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4642ba79-cca3-4ddc-8b5e-6255931af701",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
    "    # compute_metrics=compute_metrics,\n",
    "    # optimizers=(adam_bnb_optim, None),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e2fe2b-1a58-48c3-a7be-37675fb723c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tp.save_tensor_parallel(model):\n",
    "    model.save_pretrained(os.path.join(TRAINED_SAVE_PATH, \"final\"))\n",
    "    tokenizer.save_pretrained(os.path.join(TRAINED_SAVE_PATH, \"final\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b447b82f-5214-41b7-85ec-2e23271ddce6",
   "metadata": {},
   "source": [
    "# But most importantly he explayn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a1834-3688-47f2-9519-2158f9c930e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "lm_text = '<SC1>—è –∫—É–ø–∏–ª [iphone 12X]<extra_id_0> –∑–∞ [142 990]<extra_id_1> —Ä—É–± –±–µ–∑ [3-x]<extra_id_2> —á–∞—Å–æ–≤ –ø–æ–ª–¥–µ–Ω—å –∏ —Ç.–¥.'\n",
    "# lm_text = '<SC1>—è –∫—É–ø–∏–ª –∞–π—Ñ–æ–Ω –∑–∞ [14 970]<extra_id_0> —Ä—É–±–ª–µ–π'\n",
    "lm_text = \"<SC1>–í—Ä–µ–º–µ–Ω–∞–º–∏ —è –¥—É–º–∞—é, –∫–∞–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–∞–π—Ç–∏ —Ç–µ–º [14 697]<extra_id_0> —Ä—É–±–ª—è–º, —á—Ç–æ –ª–µ–∂–∞—Ç —É–∂–µ –±–æ–ª—å—à–µ [33]<extra_id_1> –ª–µ—Ç?\"\n",
    "# lm_text = \"<SC1>–ë—ã–ª–æ —É –æ—Ç—Ü–∞ [3]<extra_id_0> —Å—ã–Ω–∞, –Ω–æ –Ω–µ –±—ã–ª–æ –¥–∞–∂–µ [2-3]<extra_id_1> –ø–∏–¥–∂–∞–∫–æ–≤ —Å –±–ª—ë—Å—Ç–∫–∞–º–∏ –∑–∞ [142 990 —Ä—É–±]<extra_id_2>.\"\n",
    "# lm_text = \"<SC1>–í —à–∫–æ–ª–µ —É –º–µ–Ω—è –æ–¥–Ω–∏ [5]<extra_id_0>.\"\n",
    "# lm_text = '<SC1>–ë—ã–ª–æ —É –æ—Ç—Ü–∞ [3]<extra_id_0> —Å—ã–Ω–∞. –°—Ç–∞—Ä—à–µ–º—É –±—ã–ª–æ [35]<extra_id_1>, —Å—Ä–µ–¥–Ω–µ–º—É - –Ω–µ –º–µ–Ω—å—à–µ [33]<extra_id_2>, –∞ –º–ª–∞–¥—à–∏–π –Ω–∞ [4]<extra_id_3> –º–ª–∞–¥—à–µ –≤—Å–µ—Ö. –ë—ã–≤–∞–µ—Ç.'\n",
    "lm_text = \"<SC1>–í—Ä–µ–º–µ–Ω–∞–º–∏ —è –¥—É–º–∞—é, –∫–∞–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–∞–π—Ç–∏ —Ç–µ–º [265 948 697]<extra_id_0> —Ä—É–±–ª—è–º, —á—Ç–æ –ª–µ–∂–∞—Ç —É–∂–µ –±–æ–ª—å—à–µ [33]<extra_id_1> –ª–µ—Ç?\"\n",
    "input_ids = torch.tensor([tokenizer.encode(lm_text)]).to(\"cuda:0\")\n",
    "outputs = model.generate(input_ids, eos_token_id=tokenizer.eos_token_id, early_stopping=True)\n",
    "print(tokenizer.decode(outputs[0][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d7a17-ee66-4e45-844b-07b7cae253b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
