{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b1b475-1644-468a-afab-95306a4d69df",
   "metadata": {},
   "source": [
    "It is the time to thain something finally.\n",
    "\n",
    "Based on [translation.ipynb](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/translation.ipynb) and [fred-t5 finetune repo](https://github.com/Den4ikAI/FRED-T5-Finetuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9674bb5-a85e-498b-82b3-68da63a67ce8",
   "metadata": {},
   "source": [
    "I use a single RTX3060 12GB as naive use of 2+ GPUs cause OOM in case of `FRED-T5-large`.\n",
    "`ruT5-base` training is possible with `CUDA_VISIBLE_DEVICES=2,3` out of the box though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf75a0-c419-447a-ae82-7d4d38821c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3edf4a5-488c-494c-b520-acd70b9028e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c47bb-4eba-473b-b609-600e2b54bc61",
   "metadata": {},
   "source": [
    "I use a part of the data I have only as the model trains too long otherwise.\n",
    "8-12 hours of finetuning was just fine for my usual task so I prefer to hold on to this here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce617a23-6133-47e7-93fe-7e2f74c943d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FILES = [\n",
    "    \"/home/jovyan/data/kaggle.jsonl\",\n",
    "    \"/home/jovyan/data/ficbook_replaces.jsonl\",\n",
    "    \"/home/jovyan/data/pikabu_replaces.jsonl\",\n",
    "    # \"/home/jovyan/data/librusec_replaces.jsonl\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca096ee-0ec4-4b29-b4d3-c3a8e21db0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = {\n",
    "    0: {\n",
    "        \"type\": \"FRED-T5\",\n",
    "        \"path\": \"/home/jovyan/wdc1/models/FRED-T5-large\",\n",
    "        # \"path\": \"/home/jovyan/wdc1/models/FRED-T5-1.7B\"\n",
    "        # \"path\": \"/home/jovyan/models/3_fred-t5/checkpoint-11000\"\n",
    "    },\n",
    "    1: {\n",
    "        \"type\": \"ruT5\",\n",
    "        \"path\": \"/home/jovyan/wdc1/models/ruT5-base\",\n",
    "    },\n",
    "}[0]\n",
    "TRAINED_SAVE_PATH = \"/home/jovyan/models/7_fred-t5-large\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38bd9ac-f71d-4592-a6f2-fb8d50932f93",
   "metadata": {},
   "source": [
    "# He obtayn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a5649-c87d-45e0-ac24-c011e6b20cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd())))\n",
    "from replaces import Replace, Replaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98757f1-8be5-4b6a-a4d6-30709880ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for file in DATASET_FILES:\n",
    "    with open(file) as f:\n",
    "        for line in tqdm(f, desc=file):\n",
    "            dataset.append({\"replaces\": Replaces(json.loads(line)[\"replaces\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68172e55-55b3-4288-9404-646a5a730c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d573fa5c-75c0-433d-97db-bb44c6b9e782",
   "metadata": {},
   "source": [
    "Also `ruT5` sentencepiece tokenizer misses new line `\"\\n\"` symbol so ```<extra_id_0>\\n<extra_id_1>``` encodes-decodes into ```<unk> extra_id_0<unk> extra_id_1<unk>```. To not to fix its outout (a possible but painful action) [one is advised](https://github.com/google/sentencepiece/issues/101) to add the symbol explicitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b61374b-eb2c-489e-91e2-2c3a1ece0a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, T5Tokenizer, AutoTokenizer\n",
    "\n",
    "\n",
    "if MODEL[\"type\"] == \"ruT5\":\n",
    "    tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH)\n",
    "    tokenizer.add_tokens(\"\\n\")\n",
    "elif MODEL[\"type\"] == \"FRED-T5\":\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_PATH, eos_token=\"</s>\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3677846-72b3-47c2-b92c-a1a7a146006b",
   "metadata": {},
   "source": [
    "One problem about the last train iteration was deluded prediction of long numbers like `125678`.\n",
    "It could possibly happen because of tokenization of numbers if divided on parts which are not easy to operate.\n",
    "Lets check it out now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a2e95-d83d-4ea4-93e4-af39707368c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "good, wrong = [], []\n",
    "for i in range(100, 1000):\n",
    "    a = str(i)\n",
    "    ids = tokenizer.encode(a)\n",
    "    b = \"|\".join([tokenizer.decode(_) for _ in ids])\n",
    "    (wrong if a != b else good).append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03ffd0-915c-43cf-ae1a-719d2f7287f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good), len(wrong)\n",
    "# (28, 872) in case of FRED-T5, (19, 881) in case of ruT5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c25ee62-2672-4b6b-862f-de792fa2063f",
   "metadata": {},
   "source": [
    "The particular `FRED-T5-large` tokenizer splitted the majority of the three digits numbers.\n",
    "May be it would be better if numbers are forced splitted on single digits like `123456` to `1 2 3 4 5 6`.\n",
    "\n",
    "Other option is to divide numbers by three digit groups such that `1234567` would turn into `1 234 567`. We try that option first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d00c8d-463b-46f3-a447-c5a26036885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_numbers(s):\n",
    "    return \" \".join(((\" \".join(part) if part.isdigit() else part) for part in s.split()))\n",
    "\n",
    "\n",
    "def strip_numbers(s):\n",
    "    result = []\n",
    "    for part in s.split():\n",
    "        if part.isdigit():\n",
    "            while len(part) > 3:\n",
    "                result.append(part[:- 3 * ((len(part) - 1) // 3)])\n",
    "                part = part[- 3 * ((len(part) - 1) // 3):]\n",
    "            if part:\n",
    "                result.append(part)\n",
    "        else:\n",
    "            result.append(part)\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "strip_numbers(\"у нас было 1234567890 пакетиков травы, 750 ампул новокаина, 55555 пакетиков диэтиламида лизергиновой кислоты, солонка, на 1000/2000 наполненная кокаином\")\n",
    "# \"у нас было 1 234 567 890 пакетиков травы, 750 ампул новокаина, 55 555 пакетиков диэтиламида лизергиновой кислоты, солонка, на 1000/2000 наполненная кокаином\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb3d93-4339-4f06-b41c-5a37ea0dd5f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "data = []\n",
    "added = Counter()\n",
    "for elem in tqdm(dataset):\n",
    "    if all(_.type == \"E\" for _ in elem[\"replaces\"]):\n",
    "        continue\n",
    "    if \"prompt\" in elem and \"target\" in elem:\n",
    "        continue\n",
    "    replace_words = list(chain(*(r.text_to.strip().lower().split() for r in elem[\"replaces\"] if r.type != \"E\")))\n",
    "    added.update(replace_words)\n",
    "    prompt, target = \"<SC1>\", \"\"\n",
    "    etid = 0\n",
    "    for r in elem[\"replaces\"]:\n",
    "        if r.type == \"E\":\n",
    "            prompt += r.text_to\n",
    "        else:\n",
    "            ws_number = len(r.text_from) - len(r.text_from.rstrip())\n",
    "            prompt += f\"[{strip_numbers(r.text_from.rstrip())}]<extra_id_{etid}>{' ' * ws_number}\"\n",
    "            target += f\"<extra_id_{etid}> {r.text_to.strip()} \\n\"\n",
    "            etid += 1\n",
    "    elem[\"prompt\"] = f\"{prompt}</s>\"\n",
    "    elem[\"target\"] = f\"{target}</s>\"\n",
    "    data.append(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e309d4-d538-40a3-af7e-9e2bb1202850",
   "metadata": {},
   "source": [
    "We made here train examples of that kind\n",
    "\n",
    "    <SC1>Временами я думаю, какое применение найти тем [14 697]<extra_id_0> рублям, что лежат уже больше [33]<extra_id_1> лет?\n",
    "\n",
    "and we want to predict a text like this\n",
    "\n",
    "    <extra_id_0> четырнадцати тысячам шестистам девяноста семи\n",
    "    <extra_id_1> тридцати трёх </s>\n",
    "\n",
    "Lets check what have we added so far like the most (un)common __words__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde1073-7c33-4537-b0cb-587ed17de332",
   "metadata": {},
   "outputs": [],
   "source": [
    "added.most_common()[:10], added.most_common()[-10:], data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadf4e99-c7ca-46d8-b86c-8ee8a67a5938",
   "metadata": {},
   "source": [
    "Besides rare mistakes it seems to be trainable on.\n",
    "\n",
    "The distribution is shifted anyway to my taste as will be shown later.\n",
    "One fast and simple thing to do about it is to iterate over and filter examples as we have seen too much of **all** the replaced words at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851a6301-7160-421d-bc04-174abfc3a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_limit = (sum(added.values()) / len(added)) ** 2  # feel free to find another heuristic\n",
    "print(occ_limit)\n",
    "added2 = Counter()\n",
    "balanced_data = []\n",
    "for elem in tqdm(data):\n",
    "    replace_words = list(chain(*[r.text_to.strip().lower().split() for r in elem[\"replaces\"] if r.type != \"E\"]))\n",
    "    if any((added2[word] < occ_limit for word in replace_words)):\n",
    "        balanced_data.append(elem)\n",
    "        added2.update(replace_words)\n",
    "len(balanced_data), len(balanced_data) / len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b9beb-a30e-4b3a-8014-dde28be310f0",
   "metadata": {},
   "source": [
    "We have gotten rid of 2/3 of the data we had had!\n",
    "Check it out visually now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f584e0d2-03d9-4972-88a1-afdc58a9a7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_regs = {\n",
    "    \"re_digits\": re.compile(r\"\\d\"),\n",
    "    \"re_digits_latin\": re.compile(r\"[a-zA-Z\\d]\"),\n",
    "    \"re_latin\": re.compile(r\"[a-zA-Z]\")\n",
    "}\n",
    "for stat_name, stat_re in stat_regs.items():\n",
    "    print(\n",
    "        stat_name,\n",
    "        len([elem for elem in tqdm(balanced_data) if any(re.search(stat_re, r.text_from) for r in elem[\"replaces\"])])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52f0fe3-b7b0-444c-b124-d3e7a9a86257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "axs = plt.subplot()\n",
    "axs.set_yscale('log')\n",
    "axs.plot([_[1] for _ in added2.most_common()[:1000]])\n",
    "axs.plot([_[1] for _ in added.most_common()[:1000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e31a12-47a8-4273-9d34-e50074d5b8fe",
   "metadata": {},
   "source": [
    "Only extra data lost so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb88cae-2d41-4d23-989f-15fd8afc6ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list(balanced_data).train_test_split(test_size=0.01)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a8d28-64e2-4758-8b7f-e930151d2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"prompt\"],\n",
    "        text_target=examples[\"target\"],\n",
    "        max_length=128,  # NB should affect memory consumption\n",
    "        truncation=True\n",
    "    )\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "dataset = dataset.map(preprocess_function, batched=True, num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9a47b-1042-4334-9692-2af334eb1265",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([\"prompt\", \"target\", \"replaces\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0abc0d-8a6e-4ff7-a45b-2da3151548c5",
   "metadata": {},
   "source": [
    "Just in case I get rid of examples with possible truncation mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66e513-dfa3-4f93-b798-6738dae70584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter([len(_[\"input_ids\"]) for _ in dataset[\"train\"]])\n",
    "sum([v for k, v in c.items() if k < 128]), c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6917cfec-1674-4658-aac6-c4f03df8705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in dataset.items():\n",
    "    dataset[k] = [_ for _ in v if 10 < len(_[\"input_ids\"]) < 126]\n",
    "{k:len(v) for k, v in dataset.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669201f4-ad10-4c6b-8eed-14e712d3683a",
   "metadata": {},
   "source": [
    "# He trayn\n",
    "\n",
    "Time to train actually as last!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c31ca-1755-4033-a45b-f2443d14ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12cc228-f9db-441e-b406-62c7dfed0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer,  DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=TRAINED_SAVE_PATH,\n",
    "    optim=\"adafactor\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    logging_first_step=True,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    # gradient_checkpointing=True,\n",
    "    gradient_checkpointing=False,\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=2,\n",
    "    save_total_limit=20,\n",
    "    num_train_epochs=2,\n",
    "    # predict_with_generate=True,\n",
    "    # fp16=True,\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    # auto_find_batch_size=True,\n",
    "    dataloader_num_workers=4,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b445baa-b12a-48f1-9dba-2601b8175c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4642ba79-cca3-4ddc-8b5e-6255931af701",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e2fe2b-1a58-48c3-a7be-37675fb723c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(os.path.join(TRAINED_SAVE_PATH, \"final\"), safe_serialization=False)\n",
    "tokenizer.save_pretrained(os.path.join(TRAINED_SAVE_PATH, \"final\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b447b82f-5214-41b7-85ec-2e23271ddce6",
   "metadata": {},
   "source": [
    "# But most importantly he explayn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a1834-3688-47f2-9519-2158f9c930e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "lm_text = '<SC1>я купил [iphone 12X]<extra_id_0> за [142 990]<extra_id_1> руб без [3-x]<extra_id_2> часов полдень и т.д.'\n",
    "lm_text = '<SC1>я купил айфон за [14 970]<extra_id_0> рублей'\n",
    "lm_text = \"<SC1>Временами я думаю, какое применение найти тем [14 697]<extra_id_0> рублям, что лежат уже больше [33]<extra_id_1> лет?\"\n",
    "lm_text = \"<SC1>Было у отца [3]<extra_id_0> сына, но не было даже [2-3]<extra_id_1> пиджаков с блёстками за [142 990 руб]<extra_id_2>.\"\n",
    "lm_text = \"<SC1>В школе у меня одни [5]<extra_id_0>.\"\n",
    "lm_text = '<SC1>Было у отца [3]<extra_id_0> сына. Старшему было [35]<extra_id_1>, среднему - не меньше [33]<extra_id_2>, а младший на [4]<extra_id_3> младше всех. Бывает.'\n",
    "lm_text = \"<SC1>Временами я думаю, какое применение найти тем [265 948 697]<extra_id_0> рублям, что лежат уже больше [33]<extra_id_1> лет у нашего [DevOps]<extra_id_2>?\"\n",
    "input_ids = torch.tensor([tokenizer.encode(lm_text)]).to(\"cuda:0\")\n",
    "outputs = model.generate(input_ids, eos_token_id=tokenizer.eos_token_id, early_stopping=True, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0][1:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
