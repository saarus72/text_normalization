{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b1b475-1644-468a-afab-95306a4d69df",
   "metadata": {},
   "source": [
    "It is the time to thain something finally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34cf75a0-c419-447a-ae82-7d4d38821c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3edf4a5-488c-494c-b520-acd70b9028e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b44495-e1b9-445a-b1c5-81436dee6820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install datasets transformers[torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c47bb-4eba-473b-b609-600e2b54bc61",
   "metadata": {},
   "source": [
    "We want to use all the data we have for the first attmept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce617a23-6133-47e7-93fe-7e2f74c943d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = [\n",
    "    \"/home/jovyan/work/dataset/ficbook_pairs.jsonl\",\n",
    "    # should be first as its index fields are str not integers\n",
    "    # otherwise field type can be specified explicitely like\n",
    "    # `features=Features({'prompt': Value('string'), 'target': Value('string')})`\n",
    "    \"/home/jovyan/work/dataset/pikabu_pairs.jsonl\",\n",
    "    \"/home/jovyan/work/dataset/librusec_pairs.jsonl\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18dcf538-7d11-49f4-a98b-d7732d9bb0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['tn', 'itn', 'orig_index', 'text_index'],\n",
       "         num_rows: 236074\n",
       "     })\n",
       " }),\n",
       " {'tn': '\\nОбед номер два прошел куда лучше.',\n",
       "  'itn': 'Обед номер два прошел куда лучше.',\n",
       "  'orig_index': 'https://ficbook.net/readfic/177065/644335',\n",
       "  'text_index': 0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Features, Value, Dataset\n",
    "\n",
    "dataset = load_dataset('json', data_files=FILES)\n",
    "# dataset = dataset['train']#.train_test_split(test_size=0.1)\n",
    "dataset, dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38bd9ac-f71d-4592-a6f2-fb8d50932f93",
   "metadata": {},
   "source": [
    "# He obtayn\n",
    "\n",
    "I decided to put a construction of train examples alongside the training code itself as\n",
    "* it is fast actually and\n",
    "* I do see the preprocessing as a part of the future model.\n",
    "\n",
    "So, here is the code.\n",
    "It finds parts of two lines which are different and construct that \"before\" and \"after\" thing.\n",
    "It filters identical pairs as well since there is nothing to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfefd134-f4d2-4177-a458-90f31ba0a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "class Replace(dict):\n",
    "    def __init__(\n",
    "        self,\n",
    "        type: str, text_from: str, text_to: Optional[str]=None,\n",
    "        *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self[\"type\"] = type\n",
    "        self[\"text_from\"] = text_from\n",
    "        self[\"text_to\"] = \"\" if not text_to else text_to\n",
    "\n",
    "    @property\n",
    "    def type(self):\n",
    "        return self[\"type\"]\n",
    "\n",
    "    @property\n",
    "    def text_from(self):\n",
    "        return self[\"text_from\"]\n",
    "\n",
    "    @property\n",
    "    def text_to(self):\n",
    "        return self[\"text_to\"]\n",
    "\n",
    "    def extend(self, r):\n",
    "        if self.type != r.type:\n",
    "            raise Exception(\"Replace type mismatch\")\n",
    "        self[\"text_from\"] += r[\"text_from\"]\n",
    "        self[\"text_to\"] += r[\"text_to\"]\n",
    "\n",
    "\n",
    "class Replaces(list):\n",
    "    def add(self, r: Replace):\n",
    "        if self and r.type == self[-1].type:\n",
    "            self[-1].extend(r)\n",
    "        else:\n",
    "            return super().append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af95db67-c38a-4b33-af73-e48087dd7580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ты',\n",
       " ', ',\n",
       " 'да ',\n",
       " 'я',\n",
       " ', ',\n",
       " 'да ',\n",
       " 'мы ',\n",
       " 'c ',\n",
       " 'тобой ',\n",
       " '- ',\n",
       " 'вместе ',\n",
       " '2',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    return re.findall(r\"[а-яА-Я]+\\s*|\\d+\\s*|[^а-яА-Я\\d\\s]+\\s*\", text)\n",
    "\n",
    "\n",
    "tokenize(\"ты, да я, да мы c тобой - вместе 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a9c20bf-ba51-4866-8d05-28984a9562ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'R', 'text_from': '', 'text_to': '1'},\n",
       " {'type': 'E', 'text_from': '23', 'text_to': '23'},\n",
       " {'type': 'R', 'text_from': '4', 'text_to': '53'},\n",
       " {'type': 'E', 'text_from': '678', 'text_to': '678'},\n",
       " {'type': 'R', 'text_from': '', 'text_to': '8'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_digits = re.compile(r\"\\d\")\n",
    "\n",
    "\n",
    "def diff(seq1, seq2):\n",
    "    sm = SequenceMatcher(\n",
    "        lambda x: not re.search(r\"\\w\", x.strip()),\n",
    "        a=seq1,\n",
    "        b=seq2,\n",
    "        autojunk=False\n",
    "    )\n",
    "    result = Replaces()\n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        # print(tag, \" \".join(seq1[i1:i2]), \" \".join(seq2[j1:j2]))\n",
    "        text_from, text_to = \"\".join(seq1[i1:i2]), \"\".join(seq2[j1:j2])\n",
    "        if tag == \"equal\":\n",
    "            type = \"E\"\n",
    "        elif tag == \"replace\" and \"\".join((_.strip() for _ in seq1[i1:i2])) == \"\".join((_.strip() for _ in seq2[j1:j2])):\n",
    "            type = \"E\"\n",
    "        else:\n",
    "            if not re.search(re_digits, text_from) and not re.search(re_digits, text_to):\n",
    "                type = \"E\"\n",
    "                text_to = None\n",
    "            else:\n",
    "                type = \"R\"\n",
    "        result.add(Replace(type, text_from, text_to))\n",
    "    return result\n",
    "\n",
    "\n",
    "diff(\"234678\", \"123536788\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f56091f2-3ed4-4b81-ac5b-fe0b338f398f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tn': 'А пока я расскажу, что происходило с тобой с двадцать пятого мая.',\n",
       " 'itn': 'А пока я расскажу, что происходило с тобой с 25.05 .',\n",
       " 'orig_index': 'https://ficbook.net/readfic/17915/38547',\n",
       " 'text_index': 14,\n",
       " 'replaces': [{'type': 'E',\n",
       "   'text_from': 'А пока я расскажу, что происходило с тобой с ',\n",
       "   'text_to': 'А пока я расскажу, что происходило с тобой с '},\n",
       "  {'type': 'R', 'text_from': 'двадцать пятого мая.', 'text_to': '25.05 .'}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elem = {\"tn\": \"А пока я расскажу, что происходило с тобой с двадцать пятого мая.\", \"itn\": \"А пока я расскажу, что происходило с тобой с 25.05 .\", \"orig_index\": \"https://ficbook.net/readfic/17915/38547\", \"text_index\": 14}\n",
    "elem[\"replaces\"] = diff(tokenize(elem[\"tn\"]), tokenize(elem[\"itn\"]))\n",
    "elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33bb3d93-4339-4f06-b41c-5a37ea0dd5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0017197132110595703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 236074,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7052da47a8470c8f1251c2ab6a78a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236074 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(176270,\n",
       " {'prompt': '<SC1>- Ладно, Гезилл, буду через [15]<extra_id_0> минут. -',\n",
       "  'target': '<extra_id_0> пятнадцать '})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_good = []\n",
    "for elem in tqdm(dataset[\"train\"]):\n",
    "    elem[\"replaces\"] = diff(tokenize(elem[\"tn\"]), tokenize(elem[\"itn\"]))\n",
    "    if all(_.type == \"E\" for _ in elem[\"replaces\"]):\n",
    "        continue\n",
    "    prompt, target = \"<SC1>\", \"\"\n",
    "    etid = 0\n",
    "    for r in elem[\"replaces\"]:\n",
    "        if r.type == \"E\":\n",
    "            prompt += r.text_from\n",
    "        else:\n",
    "            ws_number = len(r.text_to) - len(r.text_to.rstrip())\n",
    "            prompt += f\"[{r.text_to.rstrip()}]<extra_id_{etid}>{' ' * ws_number}\"\n",
    "            target += f\"<extra_id_{etid}> {r.text_from.strip()} \"\n",
    "            etid += 1\n",
    "        elem[\"prompt\"] = prompt\n",
    "        elem[\"target\"] = target\n",
    "    data_good.append({\"prompt\": prompt, \"target\": target})\n",
    "len(data_good), random.choice(data_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e309d4-d538-40a3-af7e-9e2bb1202850",
   "metadata": {},
   "source": [
    "We made here train examples of that kind\n",
    "\n",
    "    <SC1>Временами я думаю, какое применение найти тем [14697]<extra_id_0> рублям, что лежат уже больше [33]<extra_id_1> лет?\n",
    "\n",
    "and we want to predict a text like this\n",
    "\n",
    "    <extra_id_0> четырнадцати тысячам шестистам девяноста семи <extra_id_1> тридцати трёх </s>\n",
    "\n",
    "There is a mess with spaces along punctuation also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffb88cae-2d41-4d23-989f-15fd8afc6ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'target'],\n",
       "        num_rows: 158643\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'target'],\n",
       "        num_rows: 17627\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_list([_ for _ in data_good]).train_test_split(test_size=0.1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669201f4-ad10-4c6b-8eed-14e712d3683a",
   "metadata": {},
   "source": [
    "# He trayn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ca096ee-0ec4-4b29-b4d3-c3a8e21db0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = \"/home/jovyan/wdc1/models/FRED-T5-1.7B\"\n",
    "MODEL_PATH = \"/home/jovyan/wdc1/models/FRED-T5-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "296c31ca-1755-4033-a45b-f2443d14ba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_PATH, eos_token='</s>')\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04180ce4-6d28-420b-b702-710fa49864b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install datasets transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d088a67-5079-4c2a-9924-3ce80098e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# path = \"./ruT5-base\"\n",
    "# model = T5ForConditionalGeneration.from_pretrained(path)\n",
    "# tokenizer = T5Tokenizer.from_pretrained(path,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b6a8d28-64e2-4758-8b7f-e930151d2bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0018832683563232422,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 158643,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057324ba94a542439a2c39349a121f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/158643 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0017712116241455078,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 17627,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ecd65be5d049148c44ca473d4d024d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"prompt\"],\n",
    "        text_target=examples[\"target\"],\n",
    "        max_length=128,  # NB should affect memory consumption\n",
    "        truncation=True\n",
    "    )\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9d9a47b-1042-4334-9692-2af334eb1265",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([\"prompt\", \"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0abc0d-8a6e-4ff7-a45b-2da3151548c5",
   "metadata": {},
   "source": [
    "Just in case I get rid of examples with possible truncation mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d66e513-dfa3-4f93-b798-6738dae70584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145944,\n",
       " Counter({128: 12699,\n",
       "          20: 3802,\n",
       "          19: 3791,\n",
       "          21: 3761,\n",
       "          17: 3720,\n",
       "          18: 3688,\n",
       "          23: 3668,\n",
       "          22: 3659,\n",
       "          16: 3608,\n",
       "          15: 3522,\n",
       "          13: 3516,\n",
       "          14: 3502,\n",
       "          25: 3380,\n",
       "          24: 3371,\n",
       "          12: 3311,\n",
       "          26: 3171,\n",
       "          27: 3143,\n",
       "          28: 3079,\n",
       "          11: 3009,\n",
       "          29: 2952,\n",
       "          30: 2873,\n",
       "          31: 2677,\n",
       "          32: 2560,\n",
       "          33: 2452,\n",
       "          10: 2427,\n",
       "          34: 2368,\n",
       "          35: 2324,\n",
       "          36: 2164,\n",
       "          37: 1997,\n",
       "          38: 1991,\n",
       "          39: 1905,\n",
       "          40: 1787,\n",
       "          41: 1732,\n",
       "          9: 1691,\n",
       "          42: 1688,\n",
       "          43: 1548,\n",
       "          45: 1507,\n",
       "          44: 1443,\n",
       "          46: 1357,\n",
       "          47: 1228,\n",
       "          48: 1214,\n",
       "          49: 1213,\n",
       "          50: 1151,\n",
       "          52: 1118,\n",
       "          51: 1108,\n",
       "          53: 1055,\n",
       "          55: 960,\n",
       "          54: 960,\n",
       "          8: 949,\n",
       "          57: 920,\n",
       "          56: 886,\n",
       "          60: 838,\n",
       "          59: 838,\n",
       "          58: 826,\n",
       "          61: 775,\n",
       "          65: 756,\n",
       "          64: 742,\n",
       "          62: 737,\n",
       "          63: 717,\n",
       "          66: 684,\n",
       "          67: 660,\n",
       "          69: 626,\n",
       "          70: 624,\n",
       "          72: 611,\n",
       "          68: 587,\n",
       "          71: 584,\n",
       "          73: 564,\n",
       "          77: 532,\n",
       "          79: 525,\n",
       "          74: 515,\n",
       "          75: 504,\n",
       "          78: 493,\n",
       "          86: 468,\n",
       "          82: 459,\n",
       "          76: 449,\n",
       "          81: 446,\n",
       "          83: 443,\n",
       "          80: 441,\n",
       "          88: 425,\n",
       "          89: 417,\n",
       "          85: 414,\n",
       "          90: 405,\n",
       "          87: 395,\n",
       "          84: 394,\n",
       "          94: 367,\n",
       "          92: 366,\n",
       "          95: 364,\n",
       "          93: 361,\n",
       "          91: 353,\n",
       "          96: 348,\n",
       "          97: 330,\n",
       "          98: 324,\n",
       "          101: 318,\n",
       "          99: 313,\n",
       "          7: 308,\n",
       "          102: 304,\n",
       "          103: 297,\n",
       "          100: 291,\n",
       "          106: 290,\n",
       "          104: 273,\n",
       "          105: 263,\n",
       "          108: 259,\n",
       "          117: 253,\n",
       "          111: 253,\n",
       "          109: 245,\n",
       "          107: 244,\n",
       "          113: 241,\n",
       "          120: 236,\n",
       "          110: 236,\n",
       "          112: 235,\n",
       "          115: 233,\n",
       "          116: 228,\n",
       "          118: 220,\n",
       "          114: 207,\n",
       "          121: 204,\n",
       "          119: 203,\n",
       "          125: 200,\n",
       "          124: 195,\n",
       "          126: 193,\n",
       "          123: 192,\n",
       "          127: 186,\n",
       "          122: 170,\n",
       "          6: 37,\n",
       "          5: 4}))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter([len(_[\"input_ids\"]) for _ in dataset[\"train\"]])\n",
    "sum([v for k, v in c.items() if k < 128]), c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6917cfec-1674-4658-aac6-c4f03df8705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in dataset.items():\n",
    "    dataset[k] = [_ for _ in v if 10 < len(_[\"input_ids\"]) < 126]\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e12cc228-f9db-441e-b406-62c7dfed0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer,  DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/home/jovyan/work/models/1_fred-t5\",\n",
    "    optim=\"adafactor\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    logging_first_step=True,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    gradient_checkpointing=0,\n",
    "    gradient_accumulation_steps=10,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    save_total_limit=10,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    bf16=True,\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4642ba79-cca3-4ddc-8b5e-6255931af701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='17515' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   12/17515 00:17 < 8:37:51, 0.56 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e2fe2b-1a58-48c3-a7be-37675fb723c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/home/jovyan/work/models/1_fred-t5/final\", safe_serialization=False)\n",
    "tokenizer.save_pretrained(\"/home/jovyan/work/models/1_fred-t5/final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b447b82f-5214-41b7-85ec-2e23271ddce6",
   "metadata": {},
   "source": [
    "# But most importantly he explayn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a1834-3688-47f2-9519-2158f9c930e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_text = '<SC1>я купил [iphone 12X]<extra_id_0> за [142 990 руб]<extra_id_1> без [3-x]<extra_id_2> часов полдень и т.д.'\n",
    "# lm_text = '<SC1>я купил айфон за [14 970]<extra_id_0> рублей'\n",
    "# lm_text = \"<SC1>Временами я думаю, какое применение найти тем [14 697]<extra_id_0> рублям, что лежат уже больше [33]<extra_id_1> лет?\"\n",
    "lm_text = \"<SC1>Было у отца [3]<extra_id_0> сына, но не было даже [2-3]<extra_id_1> пиджаков с блёстками за [142 990 руб]<extra_id_2>.\"\n",
    "# lm_text = \"<SC1>В школе у меня одни [5]<extra_id_0>.\"\n",
    "# lm_text = '<SC1>Было у отца [3]<extra_id_0> сына. Старшему было [35]<extra_id_1>, среднему - не меньше [33]<extra_id_2>, а младший на [4]<extra_id_4> младше всех. Бывает.'\n",
    "input_ids = torch.tensor([tokenizer.encode(lm_text)]).to(\"cuda:0\")\n",
    "outputs = model.generate(input_ids,eos_token_id=tokenizer.eos_token_id,early_stopping=True)\n",
    "print(tokenizer.decode(outputs[0][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d7a17-ee66-4e45-844b-07b7cae253b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
